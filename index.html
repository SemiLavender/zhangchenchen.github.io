<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="Solar">
<meta property="og:url" content="https://zhangchenchen.github.io/index.html">
<meta property="og:site_name" content="Solar">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Solar">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangchenchen.github.io/"/>





  <title> Solar </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-92407570-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Solar</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2019/08/18/Asking-the-right-question-reading-note/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/08/18/Asking-the-right-question-reading-note/" itemprop="url">
                  读书笔记-- 学会提问
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-18T16:19:30+08:00">
                2019-08-18
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/08/18/Asking-the-right-question-reading-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/08/18/Asking-the-right-question-reading-note/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>起因是一次比较失败的交流，因为很多东西没有去真正的思考过，所以很多时候没有让对话起到对话的作用，自己是那个被动接受的一方，输出者也因为没有收到对方的呼应而没有产生相应的共鸣，这是一个交流失败的典型。<br>后来就去想了想，两方面的原因吧：一是自己缺乏主动思考的意识，二是自己思考的方式不对。于是搜了一下相关书籍，找到这一本书，两天的时间读完，以此记录。</p>
<h2 id="学会提问的道"><a href="#学会提问的道" class="headerlink" title="学会提问的道"></a>学会提问的道</h2><p>这本书最精华的部分是第一章，可以理解为这是整本书的内核，可以理解为这本书的“道”—-就是批判性思维。<br>什么是批判性思维：批判性思维是有一套相互联系，环环相扣的关键问题的意识，同时加上在适当的时间提出和回答这些问题的能力。</p>
<h2 id="学会提问的术"><a href="#学会提问的术" class="headerlink" title="学会提问的术"></a>学会提问的术</h2><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://book.douban.com/subject/1829747/" target="_blank" rel="noopener">Asking the Right Questions</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/09/26/deep-in-spark-shuffle/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/09/26/deep-in-spark-shuffle/" itemprop="url">
                  Inf-- 深入理解 Spark shuffle 
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-26T21:46:30+08:00">
                2018-09-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/26/deep-in-spark-shuffle/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/09/26/deep-in-spark-shuffle/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>按以下组织方式行文，</p>
<ul>
<li>shuffle 概述。</li>
<li>spark shuffle 的大致原理。</li>
<li>spark shuffle 源码解析。</li>
<li>hadoop 的shuffle 与spark shuffle的异同。</li>
<li>spark shuffle 调优。</li>
</ul>
<h2 id="shuffle-概述"><a href="#shuffle-概述" class="headerlink" title="shuffle 概述"></a>shuffle 概述</h2><p>spark shuffle 是spark job中某些算子触发的操作，更详细点说，当rdd依赖中出现宽依赖的时候，就会触发shuffle 操作，shuffle 操作通常会伴随着不同executor/host之间数据的复制,也正因如此，导致shuffle 的代价高以及对应的复杂性。<br>举个最简单的例子，spark 中的算子 reduceByKey，该算子会生成一个新的rdd,这个新rdd中会对父rdd中相同key的value 按照指定的函数操作形成一个新的value。复杂的地方在于，相同的key 数据可能存在于父rdd的多个partition中，这就需要我们读取所有partition 中相同key值的数据然后聚合再做计算，这就是一个典型的shuffle操作。<br>产生shuffle 的算子大致有以下三类：</p>
<ul>
<li>reparition 操作：repartition,  coalesce等</li>
<li>ByKey操作：groupByKey reduceByKey等。</li>
<li>join操作：cogroup join.</li>
</ul>
<p>shuffle 操作通常会伴随着磁盘io,数据的序列化/反序列化,网络io，这些操作相对比较耗时间，往往会成为一个分布式计算任务的瓶颈，spark 也为此花了大力气进行spark shuffle的优化。从最早的hash based shuffle 到 consolidateFiles 优化，再到1.2的默认sort based shuffle，以及最近的Tungsten-sort Based Shuffle，spark shuffle一直在不断演进。关于这部分演进内容可以参考<a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">Spark Shuffle的技术演进</a><br>整体上来说，可以分为hash based shuffle和 sort based shuffle，不过随着spark shuffle的演进，两者的界限越来越越模糊，虽然2.0版本中hash based shuffle 退出历史舞台，其实只是作为Sort Based Shuffle 的一种case出现。</p>
<h2 id="spark-shuffle-原理"><a href="#spark-shuffle-原理" class="headerlink" title="spark shuffle 原理"></a>spark shuffle 原理</h2><p>因为hash based shuffle 已经退出历史舞台，所以这里直接以spark 2.3 的sort based shuffle 为例，看下spark shuffle的原理。<br>shuffle 的整个生命周期由shuffleManager 来管理，spark 2.3中，唯一的支持方式为SortShuffleManager，SortShuffleManager 中定义了writer 和reader 对应shuffle的map 和reduce阶段。writer 有三种运行模式：</p>
<ul>
<li>BypassMergeSortShuffleWriter</li>
<li>SortShuffleWriter</li>
<li>UnsafeShuffleWriter<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/shuffle-2018-09-25.jpeg" alt="shuffle"></li>
</ul>
<p>下面分别介绍一下这三种writer 的原理：</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>首先，BypassMergeSortShuffleWriter的运行机制的触发条件如下：</p>
<ul>
<li>shuffle reduce task(即partition)数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li>
<li>没有map side aggregations。<br>note: map side aggregations是指在map端的聚合操作，通常来说一些聚合类的算子都会都map端的aggregation。不过对于groupByKey 和combineByKey， 如果设定mapSideCombine为false，就不会有 map side aggregations。</li>
</ul>
<p>触发之后，整体的处理流程大致如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/sort-shuffle-bypass.png" alt="bypass-sort-shuffle"></p>
<p>每个task 会为每一个下游的reduce task 创建一个临时文件(图中下游reduce task应该有三个，这张图直接引用的美团技术博客的，不改了)，将key按照hash 存入对应临时文件中，因为写入磁盘文件是通过Java的BufferedOutputStream实现的，BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。所以图中会有内存缓冲的概念。最后，会将所有临时文件合并成一个磁盘文件，并创建一个索引文件标识下游各个reduce task的数据在文件中的start offset与end offset。<br>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一样的，也会创建很多的临时文件（所以触发条件中会有reduce task 数量限制），只是在最后会做一个磁盘文件的合并，对于shuffle  reader 会更友好一些。</p>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><p>这种writer 思想上照抄了mapreduce 的shuffle。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/sort-shuffle-common.png" alt="sort-shuffle"></p>
<p>该模式下，数据首先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。<br>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件也是通过Java的BufferedOutputStream实现的。<br>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p>
<p>hadoop shuffle map 阶段会不断地以健值对的形式把数据输出到在内存中构造的一个环形数据结构（更有效地使用内存空间）中，这里是存在map/array中。</p>
<p>BypassMergeSortShuffleWriter 与该机制相比：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用BypassMerge机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销，当然需要满足那两个触发条件。</p>
<h3 id="UnsafeShuffleWriter"><a href="#UnsafeShuffleWriter" class="headerlink" title="UnsafeShuffleWriter"></a>UnsafeShuffleWriter</h3><p>触发条件有三个：</p>
<ul>
<li>Serializer支持relocation。这是指Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致。（目前只能使用kryoSerializer），</li>
<li>没有map side aggregations</li>
<li>shuffle reduce task(即partition)数量不能大于支持的上限(2^24)</li>
</ul>
<p>UnsafeShuffleWriter 将record序列化后插入sorter，然后对已经序列化的record进行排序，并在排序完成后写入磁盘文件作为spill file，再将多个spill file合并成一个输出文件。在合并时会基于spill file的数量和IO compression codec选择最合适的合并策略。</p>
<h2 id="spark-shuffle-源码解析"><a href="#spark-shuffle-源码解析" class="headerlink" title="spark shuffle 源码解析"></a>spark shuffle 源码解析</h2><h2 id="spark-shuffle-调优"><a href="#spark-shuffle-调优" class="headerlink" title="spark shuffle 调优"></a>spark shuffle 调优</h2><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.youtube.com/watch?v=fm3Hgxuz2TM" target="_blank" rel="noopener">SOS - Optimizing Shuffle</a></p>
<p><a href="https://stackoverflow.com/questions/31283932/map-side-aggregation-in-spark" target="_blank" rel="noopener">‘map-side’ aggregation in Spark</a></p>
<p><a href="https://www.zhihu.com/question/27643595/answer/293665719" target="_blank" rel="noopener">spark的shuffle和Hadoop的shuffle（mapreduce)的区别和关系是什么？</a></p>
<p><a href="https://www.jianshu.com/p/500e8976642f" target="_blank" rel="noopener">Spark 2.1.0 - Shuffle逻辑分析</a></p>
<p><a href="https://blog.csdn.net/u010697988/article/details/70173104" target="_blank" rel="noopener">hadoop的mapReduce和Spark的shuffle过程的详解与对比及优化</a></p>
<p><a href="https://tech.meituan.com/spark_tuning_pro.html" target="_blank" rel="noopener">Spark性能优化指南——高级篇</a></p>
<p><a href="http://sharkdtu.com/posts/spark-shuffle.html" target="_blank" rel="noopener">Spark Shuffle原理及相关调优</a></p>
<p><a href="https://toutiao.io/posts/eicdjo/preview" target="_blank" rel="noopener">彻底搞懂 Spark 的 shuffle 过程（shuffle write）</a></p>
<p><a href="https://spark.apache.org/docs/2.1.1/programming-guide.html#shuffle-operations" target="_blank" rel="noopener">Shuffle operations</a></p>
<p><a href="http://hydronitrogen.com/apache-spark-shuffles-explained-in-depth.html" target="_blank" rel="noopener">Apache Spark Shuffles Explained In Depth</a></p>
<p><a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">Spark Shuffle的技术演进</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/09/16/deep-in-spark2.3-event-system/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/09/16/deep-in-spark2.3-event-system/" itemprop="url">
                  Inf-- Spark2.3 的事件体系
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-16T21:06:10+08:00">
                2018-09-16
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/16/deep-in-spark2.3-event-system/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/09/16/deep-in-spark2.3-event-system/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>换组了，工作方向由私有云转向大数据的基础架构了，感觉跟刚进公司一样，什么都得一点点慢慢积累。第一周大致过了一遍wiki,发现技术栈是完全不同，略微有点心累，不过还好，这个跳出舒适圈的决定，个人感觉是值得的。<br>进入正题，第一个小项目跟spark 有关，需要自己实现一个spark listener 来获取更多的metrics, 顺着 listener 把spark 内部的基于event 的消息总线机制过了一遍。这里做一下记录。</p>
<h2 id="事件体系相关类视图"><a href="#事件体系相关类视图" class="headerlink" title="事件体系相关类视图"></a>事件体系相关类视图</h2><p>spark 中的事件机制本质上就是观察者模式的实现。只不过封装的层次太多，不容易发现，下图是相关类的整体视图呈现。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-09-16-spark-event-bus.png" alt="spark-event-class-view"></p>
<p>先理清几个概念：</p>
<ul>
<li>Event: spark 中定义的一个事件（比如一个app/job/stage/task的启动/关闭等）。</li>
<li>Listener: 监听器，会有多个，监听event 并做出响应。</li>
<li>Bus: 消息总线，负责注册 Listener, 接收并缓存event ,将event路由给对应的Listener.</li>
</ul>
<p>图中左半部分是Bus 的具体实现，右边是Listener 的具体实现，Event 图里没有体现。<br>针对每个类讲解下重点代码：</p>
<h3 id="BUS-部分"><a href="#BUS-部分" class="headerlink" title="BUS 部分"></a>BUS 部分</h3><p>首先看下消息总线的最顶层抽象： ListenerBus,源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] val listenersPlusTimers = <span class="keyword">new</span> CopyOnWriteArrayList[(L, Option[Timer])]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Marked `private[spark]` for access in tests.</span></span><br><span class="line">  <span class="keyword">private</span>[spark] def listeners = listenersPlusTimers.asScala.map(_._1).asJava</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns a CodaHale metrics Timer for measuring the listener's event processing time.</span></span><br><span class="line"><span class="comment">   * This method is intended to be overridden by subclasses.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> def <span class="title">getTimer</span><span class="params">(listener: L)</span>: Option[Timer] </span>= None</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Add a listener to listen events. This method is thread-safe and can be called in any thread.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">final</span> def <span class="title">addListener</span><span class="params">(listener: L)</span>: Unit </span>= &#123;</span><br><span class="line">    listenersPlusTimers.add((listener, getTimer(listener)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove a listener and it won't receive any events. This method is thread-safe and can be called</span></span><br><span class="line"><span class="comment">   * in any thread.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">final</span> def <span class="title">removeListener</span><span class="params">(listener: L)</span>: Unit </span>= &#123;</span><br><span class="line">    listenersPlusTimers.asScala.find(_._1 eq listener).foreach &#123; listenerAndTimer =&gt;</span><br><span class="line">      listenersPlusTimers.remove(listenerAndTimer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling</span></span><br><span class="line"><span class="comment">   * `postToAll` in the same thread for all events.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">postToAll</span><span class="params">(event: E)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">// JavaConverters can create a JIterableWrapper if we use asScala.</span></span><br><span class="line">    <span class="comment">// However, this method will be called frequently. To avoid the wrapper cost, here we use</span></span><br><span class="line">    <span class="comment">// Java Iterator directly.</span></span><br><span class="line">    val iter = listenersPlusTimers.iterator</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      val listenerAndMaybeTimer = iter.next()</span><br><span class="line">      val listener = listenerAndMaybeTimer._1</span><br><span class="line">      val maybeTimer = listenerAndMaybeTimer._2</span><br><span class="line">      val maybeTimerContext = <span class="keyword">if</span> (maybeTimer.isDefined) &#123;</span><br><span class="line">        maybeTimer.get.time()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        doPostEvent(listener, event)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="function"><span class="keyword">case</span> <span class="title">NonFatal</span><span class="params">(e)</span> </span>=&gt;</span><br><span class="line">          logError(s<span class="string">"Listener $&#123;Utils.getFormattedClassName(listener)&#125; threw an exception"</span>, e)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (maybeTimerContext != <span class="keyword">null</span>) &#123;</span><br><span class="line">          maybeTimerContext.stop()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same</span></span><br><span class="line"><span class="comment">   * thread for all listeners.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> def <span class="title">doPostEvent</span><span class="params">(listener: L, event: E)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="keyword">private</span>[spark] def findListenersByClass[T &lt;: L : ClassTag]<span class="params">()</span>: Seq[T] </span>= &#123;</span><br><span class="line">    val c = implicitly[ClassTag[T]].runtimeClass</span><br><span class="line">    listeners.asScala.filter(_.getClass == c).map(_.asInstanceOf[T]).toSeq</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先定义了一个listenersPlusTimers ，是一个线程安全的List,Listener就是注册在该List上，此外还定义了注册/注销的方法（addListener/removeListener）。<br>重点看下 postToAll方法，该方法通过调用调用doPostEvent将event广播到listenersPlusTimers里的每一个Listener中。doPostEvent方法需要到具体的实现类中实现。<br>ListenerBus有很多种实现，这里重点关注下SparkListenerBus，源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] trait SparkListenerBus</span><br><span class="line">  extends ListenerBus[SparkListenerInterface, SparkListenerEvent] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> override def <span class="title">doPostEvent</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      listener: SparkListenerInterface,</span></span></span><br><span class="line"><span class="function"><span class="params">      event: SparkListenerEvent)</span>: Unit </span>= &#123;</span><br><span class="line">    event match &#123;</span><br><span class="line">      <span class="keyword">case</span> stageSubmitted: SparkListenerStageSubmitted =&gt;</span><br><span class="line">        listener.onStageSubmitted(stageSubmitted)</span><br><span class="line">      <span class="keyword">case</span> stageCompleted: SparkListenerStageCompleted =&gt;</span><br><span class="line">        listener.onStageCompleted(stageCompleted)</span><br><span class="line">      <span class="keyword">case</span> jobStart: SparkListenerJobStart =&gt;</span><br><span class="line">        listener.onJobStart(jobStart)</span><br><span class="line">      <span class="keyword">case</span> jobEnd: SparkListenerJobEnd =&gt;</span><br><span class="line">        listener.onJobEnd(jobEnd)</span><br><span class="line">      <span class="keyword">case</span> taskStart: SparkListenerTaskStart =&gt;</span><br><span class="line">        listener.onTaskStart(taskStart)</span><br><span class="line">      <span class="keyword">case</span> taskGettingResult: SparkListenerTaskGettingResult =&gt;</span><br><span class="line">        listener.onTaskGettingResult(taskGettingResult)</span><br><span class="line">      <span class="keyword">case</span> taskEnd: SparkListenerTaskEnd =&gt;</span><br><span class="line">        listener.onTaskEnd(taskEnd)</span><br><span class="line">      <span class="keyword">case</span> environmentUpdate: SparkListenerEnvironmentUpdate =&gt;</span><br><span class="line">        listener.onEnvironmentUpdate(environmentUpdate)</span><br><span class="line">      <span class="keyword">case</span> blockManagerAdded: SparkListenerBlockManagerAdded =&gt;</span><br><span class="line">        listener.onBlockManagerAdded(blockManagerAdded)</span><br><span class="line">      <span class="keyword">case</span> blockManagerRemoved: SparkListenerBlockManagerRemoved =&gt;</span><br><span class="line">        listener.onBlockManagerRemoved(blockManagerRemoved)</span><br><span class="line">      <span class="keyword">case</span> unpersistRDD: SparkListenerUnpersistRDD =&gt;</span><br><span class="line">        listener.onUnpersistRDD(unpersistRDD)</span><br><span class="line">      <span class="keyword">case</span> applicationStart: SparkListenerApplicationStart =&gt;</span><br><span class="line">        listener.onApplicationStart(applicationStart)</span><br><span class="line">      <span class="keyword">case</span> applicationEnd: SparkListenerApplicationEnd =&gt;</span><br><span class="line">        listener.onApplicationEnd(applicationEnd)</span><br><span class="line">      <span class="keyword">case</span> metricsUpdate: SparkListenerExecutorMetricsUpdate =&gt;</span><br><span class="line">        listener.onExecutorMetricsUpdate(metricsUpdate)</span><br><span class="line">      <span class="keyword">case</span> executorAdded: SparkListenerExecutorAdded =&gt;</span><br><span class="line">        listener.onExecutorAdded(executorAdded)</span><br><span class="line">      <span class="keyword">case</span> executorRemoved: SparkListenerExecutorRemoved =&gt;</span><br><span class="line">        listener.onExecutorRemoved(executorRemoved)</span><br><span class="line">      <span class="keyword">case</span> executorBlacklisted: SparkListenerExecutorBlacklisted =&gt;</span><br><span class="line">        listener.onExecutorBlacklisted(executorBlacklisted)</span><br><span class="line">      <span class="keyword">case</span> executorUnblacklisted: SparkListenerExecutorUnblacklisted =&gt;</span><br><span class="line">        listener.onExecutorUnblacklisted(executorUnblacklisted)</span><br><span class="line">      <span class="keyword">case</span> nodeBlacklisted: SparkListenerNodeBlacklisted =&gt;</span><br><span class="line">        listener.onNodeBlacklisted(nodeBlacklisted)</span><br><span class="line">      <span class="keyword">case</span> nodeUnblacklisted: SparkListenerNodeUnblacklisted =&gt;</span><br><span class="line">        listener.onNodeUnblacklisted(nodeUnblacklisted)</span><br><span class="line">      <span class="keyword">case</span> blockUpdated: SparkListenerBlockUpdated =&gt;</span><br><span class="line">        listener.onBlockUpdated(blockUpdated)</span><br><span class="line">      <span class="keyword">case</span> speculativeTaskSubmitted: SparkListenerSpeculativeTaskSubmitted =&gt;</span><br><span class="line">        listener.onSpeculativeTaskSubmitted(speculativeTaskSubmitted)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; listener.onOtherEvent(event)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出，SparkListenerBus定义了Listener以及Event的类型，重写了ListenerBus的doPostEvent方法，，在doPostEvent方法中通过类型匹配，只处理匹配到的事件。</p>
<p>再看下AsyncEventQueue的代码部分：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> class <span class="title">AsyncEventQueue</span><span class="params">(val name: String, conf: SparkConf, metrics: LiveListenerBusMetrics)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerBus</span></span><br><span class="line"><span class="function">  with Logging </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> AsyncEventQueue._</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Cap the capacity of the queue so we get an explicit error (rather than an OOM exception) if</span></span><br><span class="line">  <span class="comment">// it's perpetually being added to more quickly than it's being drained.</span></span><br><span class="line">  <span class="keyword">private</span> val eventQueue = <span class="keyword">new</span> LinkedBlockingQueue[SparkListenerEvent](</span><br><span class="line">    conf.get(LISTENER_BUS_EVENT_QUEUE_CAPACITY))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Keep the event count separately, so that waitUntilEmpty() can be implemented properly;</span></span><br><span class="line">  <span class="comment">// this allows that method to return only when the events in the queue have been fully</span></span><br><span class="line">  <span class="comment">// processed (instead of just dequeued).</span></span><br><span class="line">  <span class="keyword">private</span> val eventCount = <span class="keyword">new</span> AtomicLong()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** A counter for dropped events. It will be reset every time we log it. */</span></span><br><span class="line">  <span class="keyword">private</span> val droppedEventsCounter = <span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** When `droppedEventsCounter` was logged last time in milliseconds. */</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> var lastReportTimestamp = <span class="number">0L</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val logDroppedEvent = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> var sc: SparkContext = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val started = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>)</span><br><span class="line">  <span class="keyword">private</span> val stopped = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val droppedEvents = metrics.metricRegistry.counter(s<span class="string">"queue.$name.numDroppedEvents"</span>)</span><br><span class="line">  <span class="keyword">private</span> val processingTime = metrics.metricRegistry.timer(s<span class="string">"queue.$name.listenerProcessingTime"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Remove the queue size gauge first, in case it was created by a previous incarnation of</span></span><br><span class="line">  <span class="comment">// this queue that was removed from the listener bus.</span></span><br><span class="line">  metrics.metricRegistry.remove(s<span class="string">"queue.$name.size"</span>)</span><br><span class="line">  metrics.metricRegistry.register(s<span class="string">"queue.$name.size"</span>, <span class="keyword">new</span> Gauge[Int] &#123;</span><br><span class="line">    override def getValue: Int = eventQueue.size()</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val dispatchThread = <span class="keyword">new</span> Thread(s<span class="string">"spark-listener-group-$name"</span>) &#123;</span><br><span class="line">    setDaemon(<span class="keyword">true</span>)</span><br><span class="line">    <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryOrStopSparkContext(sc) &#123;</span><br><span class="line">      dispatch()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">dispatch</span><span class="params">()</span>: Unit </span>= LiveListenerBus.withinListenerThread.withValue(<span class="keyword">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      var next: SparkListenerEvent = eventQueue.take()</span><br><span class="line">      <span class="keyword">while</span> (next != POISON_PILL) &#123;</span><br><span class="line">        val ctx = processingTime.time()</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">super</span>.postToAll(next)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          ctx.stop()</span><br><span class="line">        &#125;</span><br><span class="line">        eventCount.decrementAndGet()</span><br><span class="line">        next = eventQueue.take()</span><br><span class="line">      &#125;</span><br><span class="line">      eventCount.decrementAndGet()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ie: InterruptedException =&gt;</span><br><span class="line">        logInfo(s<span class="string">"Stopping listener queue $name."</span>, ie)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">override <span class="keyword">protected</span> def <span class="title">getTimer</span><span class="params">(listener: SparkListenerInterface)</span>: Option[Timer] </span>= &#123;</span><br><span class="line">    metrics.getTimerForListenerClass(listener.getClass.asSubclass(classOf[SparkListenerInterface]))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Start an asynchronous thread to dispatch events to the underlying listeners.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> sc Used to stop the SparkContext in case the async dispatcher fails.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function">def <span class="title">start</span><span class="params">(sc: SparkContext)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span> (started.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">this</span>.sc = sc</span><br><span class="line">      dispatchThread.start()</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(s<span class="string">"$name already started!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Stop the listener bus. It will wait until the queued events have been processed, but new</span></span><br><span class="line"><span class="comment">   * events will be dropped.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function">def <span class="title">stop</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span> (!started.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(s<span class="string">"Attempted to stop $name that has not yet started!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (stopped.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">      eventCount.incrementAndGet()</span><br><span class="line">      eventQueue.put(POISON_PILL)</span><br><span class="line">    &#125;</span><br><span class="line">    dispatchThread.join()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">post</span><span class="params">(event: SparkListenerEvent)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    eventCount.incrementAndGet()</span><br><span class="line">    <span class="keyword">if</span> (eventQueue.offer(event)) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    eventCount.decrementAndGet()</span><br><span class="line">    droppedEvents.inc()</span><br><span class="line">    droppedEventsCounter.incrementAndGet()</span><br><span class="line">    <span class="keyword">if</span> (logDroppedEvent.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">      <span class="comment">// Only log the following message once to avoid duplicated annoying logs.</span></span><br><span class="line">      logError(s<span class="string">"Dropping event from queue $name. "</span> +</span><br><span class="line">        <span class="string">"This likely means one of the listeners is too slow and cannot keep up with "</span> +</span><br><span class="line">        <span class="string">"the rate at which tasks are being started by the scheduler."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    logTrace(s<span class="string">"Dropping event $event"</span>)</span><br><span class="line"></span><br><span class="line">    val droppedCount = droppedEventsCounter.get</span><br><span class="line">    <span class="keyword">if</span> (droppedCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// Don't log too frequently</span></span><br><span class="line">      <span class="keyword">if</span> (System.currentTimeMillis() - lastReportTimestamp &gt;= <span class="number">60</span> * <span class="number">1000</span>) &#123;</span><br><span class="line">        <span class="comment">// There may be multiple threads trying to decrease droppedEventsCounter.</span></span><br><span class="line">        <span class="comment">// Use "compareAndSet" to make sure only one thread can win.</span></span><br><span class="line">        <span class="comment">// And if another thread is increasing droppedEventsCounter, "compareAndSet" will fail and</span></span><br><span class="line">        <span class="comment">// then that thread will update it.</span></span><br><span class="line">        <span class="keyword">if</span> (droppedEventsCounter.compareAndSet(droppedCount, <span class="number">0</span>)) &#123;</span><br><span class="line">          val prevLastReportTimestamp = lastReportTimestamp</span><br><span class="line">          lastReportTimestamp = System.currentTimeMillis()</span><br><span class="line">          val previous = <span class="keyword">new</span> java.util.Date(prevLastReportTimestamp)</span><br><span class="line">          logWarning(s<span class="string">"Dropped $droppedCount events from $name since $previous."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * For testing only. Wait until there are no more events in the queue.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> true if the queue is empty.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">waitUntilEmpty</span><span class="params">(deadline: Long)</span>: Boolean </span>= &#123;</span><br><span class="line">    <span class="keyword">while</span> (eventCount.get() != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (System.currentTimeMillis &gt; deadline) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span></span><br><span class="line">      &#125;</span><br><span class="line">      Thread.sleep(<span class="number">10</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> object AsyncEventQueue &#123;</span><br><span class="line"></span><br><span class="line">  val POISON_PILL = <span class="keyword">new</span> SparkListenerEvent() &#123; &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先定义了一个LinkedBlockingQueue，里面缓存 Event,中间内容略过，看下 start 方法，该方法调用dispatchThread.start()，即启动一个线程调用dispatch，dispatch 会循环不停的从eventQueue中取出event，如果碰到哨兵“Event-POISON_PILL”，退出循环，该哨兵在stop  方法中添加到eventQueue的尾部。<br>再看下post方法，该方法很简单，将Event添加到eventQueue中。<br>由上可看出，这是一个典型的“生产者-消费者”模式，post方法是Event的生产者，负责向eventQueue中添加Event，dispatchThread线程是消费者，负责从eventQueue取出Event并派发到Listener。<br>spark 在将AsyncEventQueue 这个bus 放在LiveListenerBus中使用，相当于又封装了一层。看下 LiveListenerBus：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">rivate[spark] class LiveListenerBus(conf: SparkConf) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> LiveListenerBus._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> var sparkContext: SparkContext = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] val metrics = <span class="keyword">new</span> LiveListenerBusMetrics(conf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Indicate if `start()` is called</span></span><br><span class="line">  <span class="keyword">private</span> val started = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>)</span><br><span class="line">  <span class="comment">// Indicate if `stop()` is called</span></span><br><span class="line">  <span class="keyword">private</span> val stopped = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** A counter for dropped events. It will be reset every time we log it. */</span></span><br><span class="line">  <span class="keyword">private</span> val droppedEventsCounter = <span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** When `droppedEventsCounter` was logged last time in milliseconds. */</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> var lastReportTimestamp = <span class="number">0L</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val queues = <span class="keyword">new</span> CopyOnWriteArrayList[AsyncEventQueue]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Visible for testing.</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span>[scheduler] var queuedEvents = <span class="keyword">new</span> mutable.ListBuffer[SparkListenerEvent]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to queue shared by all non-internal listeners. */</span></span><br><span class="line">  <span class="function">def <span class="title">addToSharedQueue</span><span class="params">(listener: SparkListenerInterface)</span>: Unit </span>= &#123;</span><br><span class="line">    addToQueue(listener, SHARED_QUEUE)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the executor management queue. */</span></span><br><span class="line">  <span class="function">def <span class="title">addToManagementQueue</span><span class="params">(listener: SparkListenerInterface)</span>: Unit </span>= &#123;</span><br><span class="line">    addToQueue(listener, EXECUTOR_MANAGEMENT_QUEUE)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the application status queue. */</span></span><br><span class="line">  <span class="function">def <span class="title">addToStatusQueue</span><span class="params">(listener: SparkListenerInterface)</span>: Unit </span>= &#123;</span><br><span class="line">    addToQueue(listener, APP_STATUS_QUEUE)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Add a listener to the event log queue. */</span></span><br><span class="line">  <span class="function">def <span class="title">addToEventLogQueue</span><span class="params">(listener: SparkListenerInterface)</span>: Unit </span>= &#123;</span><br><span class="line">    addToQueue(listener, EVENT_LOG_QUEUE)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Add a listener to a specific queue, creating a new queue if needed. Queues are independent</span></span><br><span class="line"><span class="comment">   * of each other (each one uses a separate thread for delivering events), allowing slower</span></span><br><span class="line"><span class="comment">   * listeners to be somewhat isolated from others.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function">def <span class="title">addToQueue</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      listener: SparkListenerInterface,</span></span></span><br><span class="line"><span class="function"><span class="params">      queue: String)</span>: Unit </span>= <span class="keyword">synchronized</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"LiveListenerBus is stopped."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    queues.asScala.find(_.name == queue) match &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">Some</span><span class="params">(queue)</span> </span>=&gt;</span><br><span class="line">        queue.addListener(listener)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> None =&gt;</span><br><span class="line">        val newQueue = <span class="keyword">new</span> AsyncEventQueue(queue, conf, metrics)</span><br><span class="line">        newQueue.addListener(listener)</span><br><span class="line">        <span class="keyword">if</span> (started.get()) &#123;</span><br><span class="line">          newQueue.start(sparkContext)</span><br><span class="line">        &#125;</span><br><span class="line">        queues.add(newQueue)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">removeListener</span><span class="params">(listener: SparkListenerInterface)</span>: Unit </span>= <span class="keyword">synchronized</span> &#123;</span><br><span class="line">    <span class="comment">// Remove listener from all queues it was added to, and stop queues that have become empty.</span></span><br><span class="line">    queues.asScala</span><br><span class="line">      .filter &#123; queue =&gt;</span><br><span class="line">        queue.removeListener(listener)</span><br><span class="line">        queue.listeners.isEmpty()</span><br><span class="line">      &#125;</span><br><span class="line">      .foreach &#123; toRemove =&gt;</span><br><span class="line">        <span class="keyword">if</span> (started.get() &amp;&amp; !stopped.get()) &#123;</span><br><span class="line">          toRemove.stop()</span><br><span class="line">        &#125;</span><br><span class="line">        queues.remove(toRemove)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Post an event to all queues. */</span></span><br><span class="line">  <span class="function">def <span class="title">post</span><span class="params">(event: SparkListenerEvent)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    metrics.numEventsPosted.inc()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the event buffer is null, it means the bus has been started and we can avoid</span></span><br><span class="line">    <span class="comment">// synchronization and post events directly to the queues. This should be the most</span></span><br><span class="line">    <span class="comment">// common case during the life of the bus.</span></span><br><span class="line">    <span class="keyword">if</span> (queuedEvents == <span class="keyword">null</span>) &#123;</span><br><span class="line">      postToQueues(event)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Otherwise, need to synchronize to check whether the bus is started, to make sure the thread</span></span><br><span class="line">    <span class="comment">// calling start() picks up the new event.</span></span><br><span class="line">    <span class="keyword">synchronized</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!started.get()) &#123;</span><br><span class="line">        queuedEvents += event</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the bus was already started when the check above was made, just post directly to the</span></span><br><span class="line">    <span class="comment">// queues.</span></span><br><span class="line">    postToQueues(event)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">postToQueues</span><span class="params">(event: SparkListenerEvent)</span>: Unit </span>= &#123;</span><br><span class="line">    val it = queues.iterator()</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">      it.next().post(event)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Start sending events to attached listeners.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This first sends out all buffered events posted before this listener bus has started, then</span></span><br><span class="line"><span class="comment">   * listens for any additional events asynchronously while the listener bus is still running.</span></span><br><span class="line"><span class="comment">   * This should only be called once.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> sc Used to stop the SparkContext in case the listener thread dies.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">start</span><span class="params">(sc: SparkContext, metricsSystem: MetricsSystem)</span>: Unit </span>= <span class="keyword">synchronized</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!started.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"LiveListenerBus already started."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.sparkContext = sc</span><br><span class="line">    queues.asScala.foreach &#123; q =&gt;</span><br><span class="line">      q.start(sc)</span><br><span class="line">      queuedEvents.foreach(q.post)</span><br><span class="line">    &#125;</span><br><span class="line">    queuedEvents = <span class="keyword">null</span></span><br><span class="line">    metricsSystem.registerSource(metrics)</span><br><span class="line">  &#125;</span><br><span class="line">        ......................</span><br></pre></td></tr></table></figure>
<p>大致思想是定义了一个CopyOnWriteArrayList，这个List里面封装的就是AsyncEventQueue。之前的版本里使用的一个queue,可能是为了处理不同类型的event而分批处理，所以又加了一层。默认是有以下三个AsyncEventQueue：</p>
<ul>
<li>SHARED_QUEUE</li>
<li>APP_STATUS_QUEUE</li>
<li>EXECUTOR_MANAGEMENT_QUEUE</li>
<li>EVENT_LOG_QUEUE</li>
</ul>
<h3 id="LISTENER-部分"><a href="#LISTENER-部分" class="headerlink" title="LISTENER 部分"></a>LISTENER 部分</h3><p>这部分比较简单，直接上代码吧，首先是SparkListenerInterface：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Interface for listening to events from the Spark scheduler. Most applications should probably</span></span><br><span class="line"><span class="comment"> * extend SparkListener or SparkFirehoseListener directly, rather than implementing this class.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note that this is an internal interface which might change in different Spark releases.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] trait SparkListenerInterface &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called when a stage completes successfully or fails, with information on the completed stage.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">onStageCompleted</span><span class="params">(stageCompleted: SparkListenerStageCompleted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a stage is submitted</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onStageSubmitted</span><span class="params">(stageSubmitted: SparkListenerStageSubmitted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a task starts</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onTaskStart</span><span class="params">(taskStart: SparkListenerTaskStart)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a task begins remotely fetching its result (will not be called for tasks that do</span></span></span><br><span class="line"><span class="function"><span class="comment">   * not need to fetch the result remotely).</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onTaskGettingResult</span><span class="params">(taskGettingResult: SparkListenerTaskGettingResult)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a task ends</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onTaskEnd</span><span class="params">(taskEnd: SparkListenerTaskEnd)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a job starts</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onJobStart</span><span class="params">(jobStart: SparkListenerJobStart)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a job ends</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onJobEnd</span><span class="params">(jobEnd: SparkListenerJobEnd)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when environment properties have been updated</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onEnvironmentUpdate</span><span class="params">(environmentUpdate: SparkListenerEnvironmentUpdate)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a new block manager has joined</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onBlockManagerAdded</span><span class="params">(blockManagerAdded: SparkListenerBlockManagerAdded)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when an existing block manager has been removed</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onBlockManagerRemoved</span><span class="params">(blockManagerRemoved: SparkListenerBlockManagerRemoved)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when an RDD is manually unpersisted by the application</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onUnpersistRDD</span><span class="params">(unpersistRDD: SparkListenerUnpersistRDD)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the application starts</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onApplicationStart</span><span class="params">(applicationStart: SparkListenerApplicationStart)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the application ends</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onApplicationEnd</span><span class="params">(applicationEnd: SparkListenerApplicationEnd)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver receives task metrics from an executor in a heartbeat.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onExecutorMetricsUpdate</span><span class="params">(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver registers a new executor.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onExecutorAdded</span><span class="params">(executorAdded: SparkListenerExecutorAdded)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver removes an executor.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onExecutorRemoved</span><span class="params">(executorRemoved: SparkListenerExecutorRemoved)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver blacklists an executor for a Spark application.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onExecutorBlacklisted</span><span class="params">(executorBlacklisted: SparkListenerExecutorBlacklisted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver re-enables a previously blacklisted executor.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onExecutorUnblacklisted</span><span class="params">(executorUnblacklisted: SparkListenerExecutorUnblacklisted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver blacklists a node for a Spark application.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onNodeBlacklisted</span><span class="params">(nodeBlacklisted: SparkListenerNodeBlacklisted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver re-enables a previously blacklisted node.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onNodeUnblacklisted</span><span class="params">(nodeUnblacklisted: SparkListenerNodeUnblacklisted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when the driver receives a block update info.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onBlockUpdated</span><span class="params">(blockUpdated: SparkListenerBlockUpdated)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when a speculative task is submitted</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onSpeculativeTaskSubmitted</span><span class="params">(speculativeTask: SparkListenerSpeculativeTaskSubmitted)</span>: Unit</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment">   * Called when other events like SQL-specific events are posted.</span></span></span><br><span class="line"><span class="function"><span class="comment">   */</span></span></span><br><span class="line"><span class="function">  def <span class="title">onOtherEvent</span><span class="params">(event: SparkListenerEvent)</span>: Unit</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<p>看函数名就能看出是对应event的操作，再看一个具体的实现HeartbeatReceiver：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">private[spark] class HeartbeatReceiver(sc: SparkContext, clock: Clock)</span><br><span class="line">  extends SparkListener with ThreadSafeRpcEndpoint with Logging &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">this</span><span class="params">(sc: SparkContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(sc, <span class="keyword">new</span> SystemClock)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  sc.listenerBus.addToManagementQueue(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  override val rpcEnv: RpcEnv = sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] var scheduler: TaskScheduler = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// executor ID -&gt; timestamp of when the last heartbeat from this executor was received</span></span><br><span class="line">  <span class="keyword">private</span> val executorLastSeen = <span class="keyword">new</span> mutable.HashMap[String, Long]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// "spark.network.timeout" uses "seconds", while `spark.storage.blockManagerSlaveTimeoutMs` uses</span></span><br><span class="line">  <span class="comment">// "milliseconds"</span></span><br><span class="line">  <span class="keyword">private</span> val slaveTimeoutMs =</span><br><span class="line">    sc.conf.getTimeAsMs(<span class="string">"spark.storage.blockManagerSlaveTimeoutMs"</span>, <span class="string">"120s"</span>)</span><br><span class="line">  <span class="keyword">private</span> val executorTimeoutMs =</span><br><span class="line">    sc.conf.getTimeAsSeconds(<span class="string">"spark.network.timeout"</span>, s<span class="string">"$&#123;slaveTimeoutMs&#125;ms"</span>) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// "spark.network.timeoutInterval" uses "seconds", while</span></span><br><span class="line">  <span class="comment">// "spark.storage.blockManagerTimeoutIntervalMs" uses "milliseconds"</span></span><br><span class="line">  <span class="keyword">private</span> val timeoutIntervalMs =</span><br><span class="line">    sc.conf.getTimeAsMs(<span class="string">"spark.storage.blockManagerTimeoutIntervalMs"</span>, <span class="string">"60s"</span>)</span><br><span class="line">  <span class="keyword">private</span> val checkTimeoutIntervalMs =</span><br><span class="line">    sc.conf.getTimeAsSeconds(<span class="string">"spark.network.timeoutInterval"</span>, s<span class="string">"$&#123;timeoutIntervalMs&#125;ms"</span>) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> var timeoutCheckingTask: ScheduledFuture[_] = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// "eventLoopThread" is used to run some pretty fast actions. The actions running in it should not</span></span><br><span class="line">  <span class="comment">// block the thread for a long time.</span></span><br><span class="line">  <span class="keyword">private</span> val eventLoopThread =</span><br><span class="line">    ThreadUtils.newDaemonSingleThreadScheduledExecutor(<span class="string">"heartbeat-receiver-event-loop-thread"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val killExecutorThread = ThreadUtils.newDaemonSingleThreadExecutor(<span class="string">"kill-executor-thread"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function">override def <span class="title">onStart</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    timeoutCheckingTask = eventLoopThread.scheduleAtFixedRate(<span class="keyword">new</span> Runnable &#123;</span><br><span class="line">      <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryLogNonFatalError &#123;</span><br><span class="line">        Option(self).foreach(_.ask[Boolean](ExpireDeadHosts))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="number">0</span>, checkTimeoutIntervalMs, TimeUnit.MILLISECONDS)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">override def <span class="title">receiveAndReply</span><span class="params">(context: RpcCallContext)</span>: PartialFunction[Any, Unit] </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Messages sent and received locally</span></span><br><span class="line">    <span class="function"><span class="keyword">case</span> <span class="title">ExecutorRegistered</span><span class="params">(executorId)</span> </span>=&gt;</span><br><span class="line">      executorLastSeen(executorId) = clock.getTimeMillis()</span><br><span class="line">      context.reply(<span class="keyword">true</span>)</span><br><span class="line">    <span class="function"><span class="keyword">case</span> <span class="title">ExecutorRemoved</span><span class="params">(executorId)</span> </span>=&gt;</span><br><span class="line">      executorLastSeen.remove(executorId)</span><br><span class="line">      context.reply(<span class="keyword">true</span>)</span><br><span class="line">    <span class="keyword">case</span> TaskSchedulerIsSet =&gt;</span><br><span class="line">      scheduler = sc.taskScheduler</span><br><span class="line">      context.reply(<span class="keyword">true</span>)</span><br><span class="line">    <span class="keyword">case</span> ExpireDeadHosts =&gt;</span><br><span class="line">      expireDeadHosts()</span><br><span class="line">      context.reply(<span class="keyword">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Messages received from executors</span></span><br><span class="line">    <span class="keyword">case</span> heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (scheduler != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (executorLastSeen.contains(executorId)) &#123;</span><br><span class="line">          executorLastSeen(executorId) = clock.getTimeMillis()</span><br><span class="line">          eventLoopThread.submit(<span class="keyword">new</span> Runnable &#123;</span><br><span class="line">            <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= Utils.tryLogNonFatalError &#123;</span><br><span class="line">              val unknownExecutor = !scheduler.executorHeartbeatReceived(</span><br><span class="line">                executorId, accumUpdates, blockManagerId)</span><br><span class="line">              val response = HeartbeatResponse(reregisterBlockManager = unknownExecutor)</span><br><span class="line">              context.reply(response)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// This may happen if we get an executor's in-flight heartbeat immediately</span></span><br><span class="line">          <span class="comment">// after we just removed it. It's not really an error condition so we should</span></span><br><span class="line">          <span class="comment">// not log warning here. Otherwise there may be a lot of noise especially if</span></span><br><span class="line">          <span class="comment">// we explicitly remove executors (SPARK-4134).</span></span><br><span class="line">          logDebug(s<span class="string">"Received heartbeat from unknown executor $executorId"</span>)</span><br><span class="line">          context.reply(HeartbeatResponse(reregisterBlockManager = <span class="keyword">true</span>))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Because Executor will sleep several seconds before sending the first "Heartbeat", this</span></span><br><span class="line">        <span class="comment">// case rarely happens. However, if it really happens, log it and ask the executor to</span></span><br><span class="line">        <span class="comment">// register itself again.</span></span><br><span class="line">        logWarning(s<span class="string">"Dropping $heartbeat because TaskScheduler is not ready yet"</span>)</span><br><span class="line">        context.reply(HeartbeatResponse(reregisterBlockManager = <span class="keyword">true</span>))</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Send ExecutorRegistered to the event loop to add a new executor. Only for test.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> if HeartbeatReceiver is stopped, return None. Otherwise, return a Some(Future) that</span></span><br><span class="line"><span class="comment">   *         indicate if this operation is successful.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">addExecutor</span><span class="params">(executorId: String)</span>: Option[Future[Boolean]] </span>= &#123;</span><br><span class="line">    Option(self).map(_.ask[Boolean](ExecutorRegistered(executorId)))</span><br><span class="line">  &#125;</span><br><span class="line">     ............</span><br></pre></td></tr></table></figure>
<p>该Listener 主要作用是driver中获取executor心跳。</p>
<h3 id="Event-部分"><a href="#Event-部分" class="headerlink" title="Event 部分"></a>Event 部分</h3><p>这部分也很简单，上代码，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="meta">@JsonTypeInfo</span>(use = JsonTypeInfo.Id.CLASS, include = JsonTypeInfo.As.PROPERTY, property = <span class="string">"Event"</span>)</span><br><span class="line">trait SparkListenerEvent &#123;</span><br><span class="line">  <span class="comment">/* Whether output this event to the event log */</span></span><br><span class="line">  <span class="keyword">protected</span>[spark] def logEvent: Boolean = <span class="keyword">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerStageSubmitted</span><span class="params">(stageInfo: StageInfo, properties: Properties = <span class="keyword">null</span>)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerStageCompleted</span><span class="params">(stageInfo: StageInfo)</span> extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerTaskStart</span><span class="params">(stageId: Int, stageAttemptId: Int, taskInfo: TaskInfo)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerTaskGettingResult</span><span class="params">(taskInfo: TaskInfo)</span> extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerSpeculativeTaskSubmitted</span><span class="params">(stageId: Int)</span> extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerTaskEnd</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    stageId: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">    stageAttemptId: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">    taskType: String,</span></span></span><br><span class="line"><span class="function"><span class="params">    reason: TaskEndReason,</span></span></span><br><span class="line"><span class="function"><span class="params">    taskInfo: TaskInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">    // may be <span class="keyword">null</span> <span class="keyword">if</span> the task has failed</span></span></span><br><span class="line"><span class="function"><span class="params">    @Nullable taskMetrics: TaskMetrics)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerJobStart</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    jobId: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">    time: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    stageInfos: Seq[StageInfo],</span></span></span><br><span class="line"><span class="function"><span class="params">    properties: Properties = <span class="keyword">null</span>)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent </span>&#123;</span><br><span class="line">  <span class="comment">// Note: this is here for backwards-compatibility with older versions of this event which</span></span><br><span class="line">  <span class="comment">// only stored stageIds and not StageInfos:</span></span><br><span class="line">  val stageIds: Seq[Int] = stageInfos.map(_.stageId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerJobEnd</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    jobId: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">    time: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    jobResult: JobResult)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerEnvironmentUpdate</span><span class="params">(environmentDetails: Map[String, Seq[(String, String)</span>]])</span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerBlockManagerAdded</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    time: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    blockManagerId: BlockManagerId,</span></span></span><br><span class="line"><span class="function"><span class="params">    maxMem: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    maxOnHeapMem: Option[Long] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">    maxOffHeapMem: Option[Long] = None)</span> extends SparkListenerEvent </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerBlockManagerRemoved</span><span class="params">(time: Long, blockManagerId: BlockManagerId)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerUnpersistRDD</span><span class="params">(rddId: Int)</span> extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerExecutorAdded</span><span class="params">(time: Long, executorId: String, executorInfo: ExecutorInfo)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerExecutorRemoved</span><span class="params">(time: Long, executorId: String, reason: String)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerExecutorBlacklisted</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    time: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    executorId: String,</span></span></span><br><span class="line"><span class="function"><span class="params">    taskFailures: Int)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerExecutorUnblacklisted</span><span class="params">(time: Long, executorId: String)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">SparkListenerNodeBlacklisted</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    time: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">    hostId: String,</span></span></span><br><span class="line"><span class="function"><span class="params">    executorFailures: Int)</span></span></span><br><span class="line"><span class="function">  extends SparkListenerEvent</span></span><br><span class="line"><span class="function">  ....................</span></span><br></pre></td></tr></table></figure>
<p>定义的event都是基于SparkListenerEvent 这个trait创建的。</p>
<h2 id="整体过程"><a href="#整体过程" class="headerlink" title="整体过程"></a>整体过程</h2><p>下面按照代码逻辑走一遍，首先是SparkContext 创建的时候会定义总线LiveListenerBus：</p>
<p>SparkContext.scala<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// An asynchronous listener bus for Spark events</span></span><br><span class="line"><span class="keyword">private</span>[spark] def listenerBus: LiveListenerBus = _listenerBus</span><br></pre></td></tr></table></figure></p>
<p>同时还需要在SparkContext中调用addSparkListener将Listener 注册到LiveListenerBus的AsyncEventQueue中。</p>
<p>SparkContext.scala<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Register a listener to receive up-calls from events that happen during execution.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function">def <span class="title">addSparkListener</span><span class="params">(listener: SparkListenerInterface)</span> </span>&#123;</span><br><span class="line">  listenerBus.addToSharedQueue(listener)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到默认是将Listener添加到SharedQueue中。</p>
<p>之后会注册一些spark.extraListeners中指定的Listener,并启动bus:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Registers listeners specified in spark.extraListeners, then starts the listener bus.</span></span><br><span class="line"><span class="comment"> * This should be called after all internal listeners have been registered with the listener bus</span></span><br><span class="line"><span class="comment"> * (e.g. after the web UI and event logging listeners have been registered).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">setupAndStartListenerBus</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    conf.get(EXTRA_LISTENERS).foreach &#123; classNames =&gt;</span><br><span class="line">      val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf)</span><br><span class="line">      listeners.foreach &#123; listener =&gt;</span><br><span class="line">        listenerBus.addToSharedQueue(listener)</span><br><span class="line">        logInfo(s<span class="string">"Registered listener $&#123;listener.getClass().getName()&#125;"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: Exception =&gt;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        stop()</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> SparkException(s<span class="string">"Exception when registering SparkListener"</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  listenerBus.start(<span class="keyword">this</span>, _env.metricsSystem)</span><br><span class="line">  _listenerBusStarted = <span class="keyword">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样，当某个事件发生时，会调用总线LiveListenerBus的post方法将事件传入，AsyncEventQueue中的消费者线程就会拿到事件并调用对应的Listener。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://github.com/apache/spark" target="_blank" rel="noopener">spark github</a></p>
<p><a href="https://blog.csdn.net/qq_21383435/article/details/78666141" target="_blank" rel="noopener">spark学习-48-Spark的event事件监听器LiveListenerBus和特质SparkListenerBus以及特质ListenerBus</a></p>
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SQLListener.html" target="_blank" rel="noopener">SQLListener Spark Listener</a></p>
<p><a href="https://www.jianshu.com/p/7304d9c702a3" target="_blank" rel="noopener">Spark消息总线实现</a></p>
<p><a href="https://wongxingjun.github.io/2017/01/01/Spark%E4%BA%8B%E4%BB%B6%E7%9B%91%E5%90%AC%E8%AF%A6%E8%A7%A3/" target="_blank" rel="noopener">Spark事件监听详解</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/07/03/deep-in-celery/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/07/03/deep-in-celery/" itemprop="url">
                  Inf-- celery 源码大致解析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-03T14:57:10+08:00">
                2018-07-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/03/deep-in-celery/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/07/03/deep-in-celery/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>最近一直在用celery 开发一个自动化运维的项目，celery 的文档看完之后，虽然使用起来没什么问题了，不过感觉celery 对我还是一个黑盒，就想在源码级别大致了解一下celery，故有了这篇文章。</p>
<p>tips：本来是想把项目clone 下来花个半天的时间大致过一遍，结果发现celery 比我想象的要复杂很多，时间原因，放弃此法，转而去找现成的源码分析，拾人牙慧，难免纰漏。</p>
<h2 id="celery-与-kombu"><a href="#celery-与-kombu" class="headerlink" title="celery 与 kombu"></a>celery 与 kombu</h2><p>python 基于AMQP的库，比较常用的有两个，pika 和 kombu。kombu 相对pika 是更高层面的抽象，pika 只支持AMQP 0.9.1协议，而kombu 抽象了中间的broker，可以支持多种broker（redis,zookeeper,mongodb等）。而且相对pika 提供了很多特性：重连策略，连接池，failover 策略等等。这些策略都是一些常用且比较重要的特性，如果用pika 的话需要自己去造轮子。<br>kombo 更像是celery 的定制库，在celery中大量使用了kombu中的概念，kombu的更底层是调用的librabbitmq 库或py-amqp库来实现AMQP 0.9.1 ，在这个层面上，pika更接近py-amqp库。这里再提一嘴，open stack 项目在kombu的基础上又针对性的封装了一层，就是著名的oslo.messaging公共库了。<br>所以，如果想研究celery，那么最好先好好研究下kombo。</p>
<h2 id="从使用探究内部原理"><a href="#从使用探究内部原理" class="headerlink" title="从使用探究内部原理"></a>从使用探究内部原理</h2><p>这里就不在源码层面一层层的分析了，可以参考下面参考文章的内容，有此类的分析。这里主要是从使用层面大致说下在使用的过程中，celery 内部做了哪些事情。<br>我们使用celery的话，可能最主要的就是两个事情，一个是创建要执行的任务，然后用app.task 进行装饰，第二个是启用celery worker 进程。这里就从这两个地方入手，大致说下celery 的实现机制。</p>
<h3 id="celery-worker"><a href="#celery-worker" class="headerlink" title="celery worker"></a>celery worker</h3><p>首先了解一个blueprint 的概念，跟flask里面的概念类似，都是一个分组的概念。celery worker 启动的时候会有两个blueprint，一个是worker，一个是consumer，这两个blueprint都包含一些bootstep,bootstep 可以认为是启动的组件,bootstep 之间是有依赖关系的，也就是说，他们的启动是有顺序的，这些bootstep 很多引用了kombu中的概念。 </p>
<p>如下是worker 的bootstep:<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/celery-blueprint.png" alt="celery-blueprint"><br>简要介绍下各个bootstep 的作用：</p>
<p>Worker bootstep：</p>
<ul>
<li>Timer：用于执行定时任务的 Timer，和 Consumer 那里的 timer 不同</li>
<li>Hub：Eventloop 的封装对象（复用的 Kombu ）</li>
<li>Pool：构造各种执行池（线程/进程/协程）的</li>
<li>Autoscaler：用于自动增长或者 pool 中工作单元</li>
<li>StateDB：持久化 worker 重启区间的数据（只是重启）</li>
<li>Autoreloader：用于自动加载修改过的代码</li>
<li>Beat：创建 Beat 进程，不过是以子进程的形式运行（不同于命令行中以 beat 参数运行）</li>
</ul>
<p>Consumer bootstep：</p>
<ul>
<li>Connection：管理和 broker 的 Connection 连接</li>
<li>Events：用于发送监控事件</li>
<li>Agent：cell actor</li>
<li>Mingle：不同 worker 之间同步状态用的</li>
<li>Tasks：启动消息 Consumer</li>
<li>Gossip：消费来自其他 worker 的事件</li>
<li>Heart：发送心跳事件（consumer 的心跳）</li>
<li>Control：远程命令管理服务</li>
</ul>
<p>各个bootstep 的依赖关系：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-07-03-bootstep.png" alt="bootstep"></p>
<p>这些bootstep 的具体行为这里不做解释了，感兴趣的可以去看参考文章，celery这里还留了一个接口，可以让我们自己 custom bootstep，自定义一个bootstep 类，然后hook 一些worker 在不同阶段会执行的自定义动作。参考<a href="http://docs.celeryproject.org/en/latest/userguide/extending.html#blueprints" target="_blank" rel="noopener">Extensions and Bootsteps</a></p>
<h3 id="celery-task"><a href="#celery-task" class="headerlink" title="celery task"></a>celery task</h3><p>在定义task 的时候，我们经常使用这样一段类似代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@app.task</span><br><span class="line">def add(x,y):</span><br><span class="line">    return x+y</span><br></pre></td></tr></table></figure>
<p>执行代码看下这个add其实是一个task 对象，看下他的具体调用apply_async()函数，这里会分成两个分支，一个如果是指定同步，执行走同步的逻辑，异步会走异步的逻辑。中间都会经过一些判断逻辑（信号处理，失败处理，依赖处理等）然后会进入具体的执行逻辑，也就是消息体的创建以及发送，看下最终的消息体format：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">properties = &#123;</span><br><span class="line">    &apos;correlation_id&apos;: uuid task_id,</span><br><span class="line">    &apos;content_type&apos;: string mimetype,</span><br><span class="line">    &apos;content_encoding&apos;: string encoding,</span><br><span class="line"></span><br><span class="line">    # optional</span><br><span class="line">    &apos;reply_to&apos;: string queue_or_url,</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;lang&apos;: string &apos;py&apos;</span><br><span class="line">    &apos;task&apos;: string task,</span><br><span class="line">    &apos;id&apos;: uuid task_id,</span><br><span class="line">    &apos;root_id&apos;: uuid root_id,</span><br><span class="line">    &apos;parent_id&apos;: uuid parent_id,</span><br><span class="line">    &apos;group&apos;: uuid group_id,</span><br><span class="line"></span><br><span class="line">    # optional</span><br><span class="line">    &apos;meth&apos;: string method_name,</span><br><span class="line">    &apos;shadow&apos;: string alias_name,</span><br><span class="line">    &apos;eta&apos;: iso8601 ETA,</span><br><span class="line">    &apos;expires&apos;: iso8601 expires,</span><br><span class="line">    &apos;retries&apos;: int retries,</span><br><span class="line">    &apos;timelimit&apos;: (soft, hard),</span><br><span class="line">    &apos;argsrepr&apos;: str repr(args),</span><br><span class="line">    &apos;kwargsrepr&apos;: str repr(kwargs),</span><br><span class="line">    &apos;origin&apos;: str nodename,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">body = (</span><br><span class="line">    object[] args,</span><br><span class="line">    Mapping kwargs,</span><br><span class="line">    Mapping embed &#123;</span><br><span class="line">        &apos;callbacks&apos;: Signature[] callbacks,</span><br><span class="line">        &apos;errbacks&apos;: Signature[] errbacks,</span><br><span class="line">        &apos;chain&apos;: Signature[] chain,</span><br><span class="line">        &apos;chord&apos;: Signature chord_callback,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>其实还有很多比如 events 的实现，task state &amp; result 等都是值得探讨研究的，止笔于此，后续有需求再看。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://docs.celeryproject.org/projects/kombu/en/latest/introduction.html" target="_blank" rel="noopener">Kombu 4.2.1 documentation</a></p>
<p><a href="http://gtcsq.readthedocs.io/en/latest/openstack/kombu.html" target="_blank" rel="noopener">kombu和消息队列总结</a></p>
<p><a href="https://liuliqiang.info/post/celery-source-analysis-worker-execute-engine" target="_blank" rel="noopener">Celery 源码解析</a></p>
<p><a href="https://blog.csdn.net/happyAnger6/article/details/53869262" target="_blank" rel="noopener">Celery源码分析（一）-从命令执行到生成Worker</a></p>
<p><a href="https://stackoverflow.com/questions/48524536/can-anyone-please-tell-me-what-are-the-differences-between-pika-and-kombu-messag" target="_blank" rel="noopener">pika vs kombu</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/06/03/kafka-intro/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/06/03/kafka-intro/" itemprop="url">
                  Inf-- kafka  从入门到放弃
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-03T15:57:10+08:00">
                2018-06-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/06/03/kafka-intro/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/06/03/kafka-intro/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>工作中用到了kafka ,本篇博文整理一下学习到的kafka相关知识，内容多来自网上（主要是官网doc,个人博客），侵删。 </p>
<p>kafka 是Linkedin 使用Scala 编写的一个开源的分布式消息系统，当时开发的初衷是为了解决网站活动流数据（比如网页浏览，点击，驻留时间等等）的数据转发处理。因为活动流数据是实时且流量比较大，可想而知kafka 的最大特点就是高吞吐率且容易扩展。<br>kafka 的设计目标如下：</p>
<ul>
<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>
<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>
<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>
<li>同时支持离线数据处理和实时数据处理。</li>
<li>Scale out：支持在线水平扩展。</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>从不同的角度出发，kafka的作用也不同。</p>
<ul>
<li>kafka作为一个消息系统，类比redis,rabbitmq等同类产品，</li>
<li>kafka作为一个存储系统，因为进入kafak的消息都会被持久化所以可以作为一个存储系统，</li>
<li>kafka作为一个流处理系统，对于流数据，可以在kafka里多次处理，多次流转。</li>
</ul>
<p>相应的，kafka 的使用场景大致如下：</p>
<ul>
<li>作为传统消息队列系统的替换。</li>
<li>做metric监控数据的收集处理。</li>
<li>做日志数据的收集处理。</li>
<li>流数据处理。</li>
<li>事件驱动架构的核心组件。 </li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-06-04kafka-api.png" alt="kafka-api"></p>
<p>上图中可看出kafka的角色，相应的，kafka也提供了四种核心api:</p>
<ul>
<li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
<li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
<li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
<li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
</ul>
<h2 id="整体架构与术语"><a href="#整体架构与术语" class="headerlink" title="整体架构与术语"></a>整体架构与术语</h2><p>了解kafka，需要了解kafka  的一些术语,包括 producer,consumer,topic ,partition,broker等等。下面结合架构图统一解释：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-06-04-kafka-infra.png" alt="kafka-inf"></p>
<ul>
<li>首先kafka 有producer 和consumer的概念，一个生产数据，一个消费数据，中间是一个broker（也就是一个消息队列），producer是以push的方式向broker推数据，consumer以pull 的方式从broker消费数据。</li>
<li>producer 与consumer是通过topic 关联在一块的。topic类似于一个特定的分类。producer向某个指定的topic push数据，consumer从指定的topic消费数据。</li>
<li>partition 的引入是为了解决并发量的问题，一个topic （至少有一个partition）可以有多个partition，多个partition并发的处理一个topic的消息。每一个partition以顺序不变的方式保存消息。</li>
<li>broker  是kafka集群中维护发布消息的系统。每个broker针对每个topic可能包含0个或多个该topic的分区。假设，一个topic拥有N个分区，并且集群拥有N个broker,则每个broker会负责一个分区。 假设，一个topic拥有N个分区，并且集群拥有N+M个broker,则前N个broker每个处理一个分区，剩余的M个broker则不会处理任何分区 。 假设，一个topic拥有N个分区，并且集群拥有M个broker（M &lt; N），则这些分区会在所有的broker中进行均匀分配。每个broker可能会处理一个或多个分区。这种场景不推荐使用，因为会导致热点问题和负载不均衡问题。</li>
<li>Replicas of partition 分区副本仅仅是分区的备份，不会对副本分区进行读写操作，只是用来防止数据丢失。上图中topic2 就设置了 partition=2，副本会均衡地分布在broker中。</li>
<li>consumer group: 一个consumergroup 里面会有若干个consumer实例，对应一个topic，这几个consumer实例都会消费该topic 中的消息，如果consumer实例的数量等于对应topic的partition数量，那么一个consumer对应一个partition（推荐）。如果consumer实例的数量小于对应topic的partition数量，那么一个consumer可能对应多个partition。如果consumer实例的数量大于对应topic的partition数量，那么多出的consumer不会参与到topic的消息消费。</li>
</ul>
<h2 id="安装与使用"><a href="#安装与使用" class="headerlink" title="安装与使用"></a>安装与使用</h2><p>参考<a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">quick start</a></p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>kafka 依赖zookeeper来实现负载均衡以及原数据的存取。</li>
<li>kafka之所以实现如此快的数据持久化，是因为磁盘io是顺序读写，在某些情况下，顺序磁盘访问能够比随即内存访问还要快（跟操作系统的预读，后写等技术有关）。</li>
<li>kafka 的consumer group 实现了consumer 的auto rebalance，之前是依赖zookeeper实现的，会出现脑裂的情况，后来专门开发了coordinator组件来实现，目前仍在不断改进。</li>
<li>kafka 的高性能有很多原因，包括消息的batch send,消息压缩（producer端和broker端都可做），ISR（in-sync replicas)机制，磁盘 append only，page cache等等。</li>
</ul>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kafka.apache.org/documentation/#uses_logs" target="_blank" rel="noopener">kafka Documentation</a></p>
<p><a href="https://leokongwq.github.io/2017/02/06/mq-kafka-tutorial.html#" target="_blank" rel="noopener">kafka入门教程</a></p>
<p><a href="http://www.jasongj.com/2015/03/10/KafkaColumn1/" target="_blank" rel="noopener">Kafka系列文章</a></p>
<p><a href="https://www.oschina.net/translate/kafka-design" target="_blank" rel="noopener">分布式发布订阅消息系统 Kafka 架构设计</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/05/10/mysql-on-k8s-in-production/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/10/mysql-on-k8s-in-production/" itemprop="url">
                  Kuberbetes-- 部署生产级MySQL到Kubernetes集群中
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-10T11:02:11+08:00">
                2018-05-10
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/10/mysql-on-k8s-in-production/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/05/10/mysql-on-k8s-in-production/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>K8s天然支持无状态应用的自动化运维（如HA,Scale等），对于有状态应用就相对比较麻烦了，本文以Mysql为例，详细梳理一下有状态应用在K8S中的部署运维。</p>
<h2 id="Mysql-HA-方案"><a href="#Mysql-HA-方案" class="headerlink" title="Mysql HA 方案"></a>Mysql HA 方案</h2><p>有状态应用在K8S中难以运维的原因是该应用本身就比较难运维（不然就不会有DBA 这样一个专门的职位了），当涉及到有状态应用时，你要考虑数据的存储，可用性，扩展性，事务，灾备等等。不过要想在K8S中部署生产级别的有状态应用，首先要知道在没有使用K8S时生产级别的应用架构方案。<br>这里还是以Mysql为例，简单说下常用的几种生产环境中的Mysql 架构方案。<br>本人不是专业的DBA,而且本身Mysql HA就是水很深的一个方向，理解有限，这里只谈大致方案，不说技术细节。<br>Mysql HA架构有很多种，具体选型时要考虑架构的HA level，支撑应用的性质，具体的部署环境等。 这里根据Mysql DOC中的分类来具体说明（个人感觉这个分类也不是太精确），官网根据能达到的HA level分了三个层次，分别是：</p>
<ul>
<li>Data Replication.</li>
<li>Clustered &amp; Virtualized Systems.</li>
<li>Shared-Nothing, Geographically-Replicated Clusters.</li>
</ul>
<p>这三个层次的可用性依次递增，不过复杂性也随之增加。<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510142133tradeoff.jpg" alt="tradeoff"></p>
<h3 id="Data-Replication"><a href="#Data-Replication" class="headerlink" title="Data Replication"></a>Data Replication</h3><p>这个级别的架构主要是 replication的方法实现mysql数据的复制冗余，具体方案有如下几种：</p>
<h4 id="MYSQL-主从或主主半同步复制"><a href="#MYSQL-主从或主主半同步复制" class="headerlink" title="MYSQL 主从或主主半同步复制"></a>MYSQL 主从或主主半同步复制</h4><p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510150744-replication.jpg" alt="replication"></p>
<p>这种架构比较简单，使用原生半同步复制作为数据同步的依据,缺点是需要额外考虑haproxy、keepalived的高可用机制，而且完全依赖于半同步复制，如果半同步复制退化为异步复制，数据一致性无法得到保证，可以通过针对网络波动的半同步复制优化解决。</p>
<h4 id="利用MHA-实现HA"><a href="#利用MHA-实现HA" class="headerlink" title="利用MHA 实现HA"></a>利用MHA 实现HA</h4><p>MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本DeNA公司youshimaton（现就职于Facebook公司）开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。它主要解决的是master fail-over的情况下实现秒级切换且保证数据一致性。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510151622-mha1.jpg" alt="mha"></p>
<p>优点是操作非常简单，可以将任意slave提升为master，且MHA可以设定多个master,可用性提高，缺点是逻辑较为复杂，发生故障后排查问题，定位问题更加困难，且用perl开发，二次开发困难。<br>除了MHA ,还有MMM(Master-Master replication managerfor Mysql，Mysql主主复制管理器)方案。</p>
<h4 id="SAN共享存储实现HA"><a href="#SAN共享存储实现HA" class="headerlink" title="SAN共享存储实现HA"></a>SAN共享存储实现HA</h4><p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510152934-san.jpg" alt="san"></p>
<p>这种方案因为使用共享存储，不需要数据同步，不过专门的共享存储花销比较大，且需要专门的运维人员，适合一些土豪公司。</p>
<h4 id="DRBD磁盘复制实现HA"><a href="#DRBD磁盘复制实现HA" class="headerlink" title="DRBD磁盘复制实现HA"></a>DRBD磁盘复制实现HA</h4><p>DRBD是一种基于软件、基于网络的块复制存储解决方案，主要用于对服务器之间的磁盘、分区、逻辑卷等进行数据镜像，当用户将数据写入本地磁盘时，还会将数据发送到网络中另一台主机的磁盘上，这样的本地主机(主节点)与远程主机(备节点)的数据就可以保证实时同步。<br>穷人版的SAN ,唯一不同的是没有使用SAN网络存储 ，而是使用local disk。由于是实时复制磁盘数据，性能会有影响。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510153049-drbd.jpg" alt="drbd"></p>
<h3 id="Clustered-amp-Virtualized-Systems"><a href="#Clustered-amp-Virtualized-Systems" class="headerlink" title="Clustered &amp; Virtualized Systems"></a>Clustered &amp; Virtualized Systems</h3><p>大致有两种方案，一种是基于Mysql的NDB CLUSTER，另一种是MarianDB的Galera方案。</p>
<h4 id="Mysql-NDB-CLUSTER"><a href="#Mysql-NDB-CLUSTER" class="headerlink" title="Mysql NDB CLUSTER"></a>Mysql NDB CLUSTER</h4><p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510160922-ndb.jpg" alt="ndb"></p>
<p>国内用NDB集群的公司非常少。NDB集群不需要依赖第三方组件，全部都使用官方组件，能保证数据的一致性，某个数据节点挂掉，其他数据节点依然可以提供服务，管理节点需要做冗余以防挂掉。<br>缺点是：管理和配置都很复杂，而且某些SQL语句例如join语句需要避免。</p>
<h4 id="MarianDB-Galera-Cluster"><a href="#MarianDB-Galera-Cluster" class="headerlink" title="MarianDB Galera Cluster"></a>MarianDB Galera Cluster</h4><p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180510161231-galera.jpg" alt="galera"></p>
<p>MariaDB Galera Cluster 是一套在mysql innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到 各个节点上去。在数据方面完全兼容 MariaDB 和 MySQL。 </p>
<h3 id="Geographically-Replicated-Clusters"><a href="#Geographically-Replicated-Clusters" class="headerlink" title="Geographically-Replicated Clusters"></a>Geographically-Replicated Clusters</h3><p>这种级别的Mysql集群是跨地域的数据中心，其实是上述解决方案的一种scale，但是规模更大，架构更加复杂，也没有一种统一的解决方案，这里不做讨论。</p>
<h2 id="部署Mysql-到K8S集群"><a href="#部署Mysql-到K8S集群" class="headerlink" title="部署Mysql 到K8S集群"></a>部署Mysql 到K8S集群</h2><p>To be continued</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.slideshare.net/bytebot/best-practices-for-mysql-high-availability" target="_blank" rel="noopener">Best practices for MySQL High Availability </a></p>
<p><a href="http://www.clusterdb.com/mysql/choosing-the-right-mysql-high-availability-solution-webinar-replay" target="_blank" rel="noopener">Choosing the right MySQL High Availability Solution</a></p>
<p><a href="https://dev.mysql.com/doc/mysql-ha-scalability/en/ha-overview.html" target="_blank" rel="noopener">High Availability and Scalability</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25960208" target="_blank" rel="noopener">五大常见的MySQL高可用方案</a></p>
<p><a href="http://www.fblinux.com/?p=1044" target="_blank" rel="noopener">mysql复制高可用方案总结</a></p>
<p><a href="http://database.51cto.com/art/201504/473788.htm" target="_blank" rel="noopener">MySQL高可用各个技术的比较</a></p>
<p><a href="https://juejin.im/entry/59edb5656fb9a0452404fd78" target="_blank" rel="noopener">用 Go 搭建 Kubernetes Operators</a></p>
<p><a href="https://schd.ws/hosted_files/kccncna17/cc/MySQL%20on%20Kubernetes.pdf" target="_blank" rel="noopener">mysql on kubernetes</a></p>
<p><a href="http://dockone.io/article/3673" target="_blank" rel="noopener">容器化RDS：计算存储分离还是本地存储？</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/03/09/record-for-docker-storage-driver/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/09/record-for-docker-storage-driver/" itemprop="url">
                  Docker-- Docker storage driver 概述
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-09T11:57:10+08:00">
                2018-03-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/09/record-for-docker-storage-driver/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/09/record-for-docker-storage-driver/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Docker 配置的时候有一个很重要的配置项就是 storage driver选项，本篇博客详细介绍下storage driver这一配置项的相关内容。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>首先是 storage driver出现的原因。我们知道容器的存储大致有两种，一种是在容器外的，比如 volume，不会随着容器的消亡而消失，有自己的生命周期。还有一种是容器内的，这种存储跟对应容器的生命周期是紧密结合在一起的。而我们要说的就是容器内的存储。<br>本地的Docker引擎有一个Docker镜像层的缓存，镜像层是层层叠加的。当容器运行起来的时候就是在镜像层上起来的。基于同一个镜像运行的容器会共用一个镜像。那如何保证容器内操作后内容的独立呢，就要使用容器层以及写时复制（COW）技术。如下图：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309142857-cow.jpg" alt="cow"></p>
<p>最底层的基础镜像是ubuntu的系统镜像，再往上是分层的镜像层（比如dockerfile中的软件安装等），这些都是只读的镜像层。最上面才是可以读写的容器层，不同的容器有不同的容器层，共用相同的镜像层。当某个容器需要写操作时，会先将写的内容从镜像层复制到容器层，然后再写入（也就是写时复制），读的时候会从容器层开始，如果命中则读取，没有命中则依次往下读取。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309145042-storage-driver.jpg" alt="storage-driver"></p>
<p>至于以上原理的具体的实现，就是storage driver做的事。</p>
<h2 id="storage-driver-的种类以及选型"><a href="#storage-driver-的种类以及选型" class="headerlink" title="storage driver 的种类以及选型"></a>storage driver 的种类以及选型</h2><p>注：本文是基于最新版Docker（V17.12），版本不同，选择也不同，具体参考官网。</p>
<p>目前为止，常用的storage driver有以下几种：AUFS，Btrfs，Device mapper，Overlayfs，ZFS，VFS。其中，</p>
<ul>
<li>VFS是接口的“原生”的实现，完全没有使用联合文件系统或者写时复制技术，而是将所有的分层依次拷贝到静态的子文件夹中，然后将最终结果挂载到容器的根文件系统。它并不适合实际或者生产环境使用，但是对于需要进行简单验证的场景，或者需要测试Docker引擎的其他部件的场景，是很有价值的。</li>
<li>Btrfs和ZFS针对特定的文件系统，也就是 backing filesystem必须相应的是Btrfs和ZFS。</li>
<li>AUFS和Overlayfs（包括Overlayfs2）都是在原有文件系统上基于联合挂载实现，而Device mapper是所有的镜像和容器存储在它自己的虚拟设备上，这些虚拟设备是一些支持写时复制策略的快照设备。</li>
</ul>
<p>具体的选择策略要根据linux 发行版，docker版本以及文件系统来决定：</p>
<p>对于Docker 社区版本来说，不同linux发行版的选择如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309151325-linux-distribution.jpg" alt="linux-distribution"></p>
<p>对于不同的文件系统，推荐如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309151640-file-system.jpg" alt="file-system"></p>
<p>综上，选择时可以如下选择：</p>
<ul>
<li>1，如果内核支持多个storage driver，可以如下考虑优先级：首先考虑特定文件系统的storage driver,即Btrfs和ZFS。否则，可以使用稳定和通用的配置，overlay2,或device mapper（需要手动配置direct-lvm，因为默认的loopback-lvm性能一般）。</li>
<li>2，依据具体的Docker 版本, 操作系统版本做选择（见上图）。</li>
<li>3，依据具体的 backing filesystem做选择（见上图）。</li>
</ul>
<h2 id="几种典型-storage-driver的原理"><a href="#几种典型-storage-driver的原理" class="headerlink" title="几种典型 storage driver的原理"></a>几种典型 storage driver的原理</h2><p>简单讲下AUFS，Overlayfs，Device mapper实现的具体原理。</p>
<h3 id="AUFS-的实现原理"><a href="#AUFS-的实现原理" class="headerlink" title="AUFS 的实现原理"></a>AUFS 的实现原理</h3><p>AUFS是一种联合文件系统，意思是它将同一个主机下的不同目录堆叠起来(类似于栈)成为一个整体，对外提供统一的视图。AUFS是用联合挂载来做到这一点。 在Docker中，AUFS实现了镜像的分层。AUFS中的分支对应镜像中的层。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309155232-aufs.jpg" alt="aufs"></p>
<ul>
<li>aufs中文件的读写:读的时候会先去container layer读，如果没有会继续往下层读取。写的时候也是，如果container layer文件没有，先去image layer复制文件到container layer,然后再写入。因为AUFS工作在文件的层次上，也就是说AUFS对文件的操作需要将整个文件复制到读写层内，哪怕只是文件的一小部分被改变，也需要复制整个文件。</li>
<li>aufs中文件的删除:AUFS通过在最顶层(container layer)生成一个whiteout文件来删除文件。whiteout文件会掩盖下面只读层相应文件的存在，但它事实上没有被删除。</li>
</ul>
<h3 id="Overlayfs的实现原理"><a href="#Overlayfs的实现原理" class="headerlink" title="Overlayfs的实现原理"></a>Overlayfs的实现原理</h3><p>OverlayFS与AUFS相似，也是一种联合文件系统(union filesystem)，与AUFS相比，OverlayFS： 设计更简单，被加入Linux3.18版本内核 ，可能更快。</p>
<p>Overlay通过三个概念来实现它的文件系统：一个“下层目录（lower-dir）”，一个“上层目录（upper-dir）”，和一个做为文件系统合并视图的“合并（merged）”目录。受限于只有一个“下层目录”，需要额外的工作来让“下层目录”递归嵌套（下层目录自己又是另外一个overlay的联合），或者按照Docker的实现，将所有位于下层的内容都硬链接到“下层目录”中，这就可能导致inode爆炸式增长（因为有大量的分层内容和硬连接）。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309160513-overlay.jpg" alt="overlay"></p>
<p>Overlay2基于Linux内核4.0和以后版本中overlay的特性，可以允许有多个下层的目录，解决了一些因为最初驱动的设计而引发的inode耗尽和一些其他问题。不过由于代码库相对还比较年轻，有待时间的检验。</p>
<p>理论情况下，overlay2 和 overlay要比aufs 和 devicemapper性能好，甚至，一些情况下，overlay2要比btrfs好。不过也有一些需要注意的方面：</p>
<ul>
<li>OverlayFS 支持页缓存（page caching）共享。意味着多个使用同一文件的容器可以共享同一页缓存，这使得overlayfs具有很高的内存使用效率。</li>
<li>同aufs一样，第一次写文件时需要复制整个文件，这会带来一些性能开销，在修改大文件时尤其明显。 </li>
<li>overlay的inode限制。</li>
</ul>
<h3 id="Device-mapper的实现原理"><a href="#Device-mapper的实现原理" class="headerlink" title="Device mapper的实现原理"></a>Device mapper的实现原理</h3><p>device mapper将所有的镜像和容器存储在它自己的虚拟设备上，这些虚拟设备是一些支持写时复制策略的快照设备。device mapper工作在块层次上而不是文件层次上，这意味着它的写时复制策略不需要拷贝整个文件。<br>device mapper创建镜像的过程如下： </p>
<ul>
<li>使用device mapper的storge driver创建一个精简配置池；精简配置池由块设备或稀疏文件创建。 </li>
<li>接下来创建一个基础设备； </li>
<li>每个镜像和镜像层都是基础设备的快照；这写快照支持写时复制策略，这意味着它们起始都是空的，当有数据写入时才耗费空间。<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309162425-device-mapper.jpg" alt="device-mapper"><br>镜像的每一层都是它下面一层的快照，镜像最下面一层是存在于thin pool中的base device的快照。容器是创建容器的镜像的快照。<br>device mapper跟之前的storage driver最大的不同就是它是基于块而不是基于文件，所以对文件的操作实际是对对应文件块的操作，默认每个块的大小为64KB。<br>图展示了容器中的某个进程读取块号为0x44f的数据：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180309163531-device-mapper.jpg" alt="device-mapper"></li>
</ul>
<p>device mapper不是最有效使用存储空间的storage driver，启动n个相同的容器就复制了n份文件在内存中，这对内存的影响很大。所以device mapper并不适合容器密度高的场景。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://docs.docker.com/storage/storagedriver/select-storage-driver/" target="_blank" rel="noopener">Docker storage drivers</a></p>
<p><a href="https://www.centos.bz/2016/12/select-a-docker-storage-driver/" target="_blank" rel="noopener">Docker用户指南(4) – 存储驱动选择</a></p>
<p><a href="http://dockone.io/article/1765" target="_blank" rel="noopener">深入了解Docker存储驱动</a></p>
<p><a href="http://blog.csdn.net/vchy_zhao/article/details/70238690" target="_blank" rel="noopener">Docker之几种storage-driver比较</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/01/30/record-for-hdfs/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/30/record-for-hdfs/" itemprop="url">
                  Hadoop-- Hadoop学习之HDFS
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-30T14:57:10+08:00">
                2018-01-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/30/record-for-hdfs/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/01/30/record-for-hdfs/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>最近因为项目需要将重心转移到大数据架构这一块，所以将大数据的内容大致过一下，在此记录，内容大部分来自官方文档与博客，主要基于Hadoop 的最新stable版本（2.9）。<br>学习大数据，立马想到的就是Hadoop，Hadoop是一个开源的可依赖，可扩展的分布式计算框架，采用普通PC机集群完成对大量数据的存储，计算。这个项目主要包括四个模块：</p>
<ul>
<li>Hadoop Common: Hadoop的Common utilities模块。</li>
<li>Hadoop Distributed File System (HDFS): 一个支持高可用的分布式文件系统。</li>
<li>Hadoop YARN: 任务调度和资源管理框架。</li>
<li>Hadoop MapReduce: 基于YARN的对大量数据并行处理的计算模型。<br>除了以上四个主要模块之外，Hadoop生态还有很多其他的的系统，以后再慢慢写，这里先写Hadoop生态中最重要的一个组件HDFS。</li>
</ul>
<p>先看下Hadoop里的服务角色：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180201160125-hadoop-role.jpg" alt="hadoop-server-role"></p>
<p>Hadoop主要的任务部署分为3个部分，分别是：Client机器，主节点和从节点。主节点主要负责Hadoop两个关键功能模块HDFS、Map Reduce的监督。Job Tracker使用Map Reduce进行监控和调度数据的并行处理，namenode则负责HDFS监视和调度。从节点负责了机器运行的绝大部分，担当所有数据储存和指令计算的苦差。每个从节点既扮演着数据节点的角色又充当与他们主节点通信的守护进程。守护进程隶属于Job Tracker，数据节点则归属于名称节点。Client负责把数据加载到集群中，递交给Map Reduce做数据处理工作，并在工作结束后取回或者查看结果。</p>
<p>再看下典型的围绕hadoop 的workflow：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180201160825hadoop-workflow.jpg" alt="hadoop-workflow"></p>
<h2 id="HDFS-Architecture"><a href="#HDFS-Architecture" class="headerlink" title="HDFS Architecture"></a>HDFS Architecture</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>HDFS的整体设计架构就是master/slave，namenode负责元数据的存取，数据管理等功能，datanode负责数据存储。如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180201154432-hdfs-arch.jpg" alt="hdfs-arch"></p>
<h3 id="datanode工作原理"><a href="#datanode工作原理" class="headerlink" title="datanode工作原理"></a>datanode工作原理</h3><p>数据的存储方式与ceph类似，默认都是三副本，先将大数据文件切割成固定大小（比如128M）的block，然后将这些block存放到三个不同的datanode中，namenode会记录对应文件block以及block所在位置。<br>举例一个datanode存储过程:client先做文件切割，并提交存储文件命令给namenode，namenode有一个rack awareness的功能，简单点说就是会将数据存储到不同机架上以避免机架故障（电源故障等），如果是三副本的话，首先client会写入block到某一节点A，然后另一机架中的节点B 会从A复制该数据,再然后同一机架内的另一个节点C会从B复制一份数据。这样一个pipeline既保证了数据的容灾，也能减小数据传输的延迟。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180201161950-data-node.jpg" alt="datanode"></p>
<h3 id="namenode工作原理"><a href="#namenode工作原理" class="headerlink" title="namenode工作原理"></a>namenode工作原理</h3><p>理解namenode，首先要理解两个文件。一个是 Edits文件，一个是FsImage映像文件，这两个文件就包含整个HDFS集群的元数据，而namenode的最大任务就是维护这两个文件。</p>
<ul>
<li>Edits文件：NameNode在本地操作系统的文件都会保存在Edits日志文件中。也就是说当文件系统中的任何元数据产生操作时，都会记录在Edits日志文件中。eg：在HDFS上创建一个文件，NameNode就会在Edits中插入一条记录。同样如果修改或者删除等操作，也会在Edits日志文件中新增一条数据。</li>
<li>FsImage映像文件：整个文件系统的名字空间，包括数据块到文件的映射，文件的属性等等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的文件系统中。（注意到该文件中并没有blockmap,描述数据块Block与DataNode节点之间的对应关系的文件，这是因为每个DataNode已经持有属于自己管理的Block集合，每次blockreport都会将所有DataNode的Block集合汇总后即可构造出完整BlocksMap，所以不用持久化。）</li>
</ul>
<p>为什么会引入这两个文件呢，因为在HDFS的整个运行期里，所有元数据均在NameNode的内存集中管理，但是由于内存易失特性，一旦出现进程退出、宕机等异常情况，所有元数据都会丢失，为了更好的容错能力，NameNode会周期进行Checkpoint，将其中的一部分元数据（文件系统的目录树Namespace）刷到持久化设备上，即二进制文件FSImage，这样的话即使NameNode出现异常也能从持久化设备上恢复元数据。但是仅周期进行Checkpoint仍然无法保证所有数据的可靠，如前次Checkpoint之后写入的数据依然存在丢失的问题，所以将两次Checkpoint之间对Namespace写操作实时写入EditLog文件，通过这种方式可以保证HDFS元数据的绝对安全可靠。</p>
<p>首先看一下namenode启动时做了哪些操作（下文部分直接引用自<a href="http://blog.csdn.net/mmd0308/article/details/74674524" target="_blank" rel="noopener">该博客</a>）：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180201163924-namenode.jpg" alt="namenode"></p>
<p>接下来看看check point的时候做了哪些操作,这个时候就要引入Secondary NameNode了，NameNode主要是存储文件的metadata，运行时所有数据都保存在内存中，这个的HDFS可存储的文件受限于NameNode的内存。而Secondary NameNode可以看做是NameNode的灾备（并非HA），它会定时与NameNode进行同步，定期的将fsimage映像文件和Edits日志文件进行合并，并将合并后的传入给NameNode，替换其镜像，并清空编辑日志。如果NameNode失效，需要手动的将其设置成namenode主机。<br>checkpoint的时间默认是3600秒（可配置），当Edits日志文件超过最大值时也会进行check point。checkpoint大致如下：</p>
<ul>
<li>NameNode通知Secondary NameNode进行checkpoint。</li>
<li>Secondary NameNode通知NameNode切换edits日志文件，使用一个空的。</li>
<li>Secondary NameNode通过Http获取NmaeNode上的fsimage映像文件和切换前的edits日志文件。</li>
<li>Secondary NameNode在内容中合并fsimage和Edits文件。</li>
<li>Secondary NameNode将合并之后的fsimage文件发送给NameNode。</li>
<li>NameNode用Secondary NameNode 传来的fsImage文件替换原先的fsImage文件</li>
</ul>
<h2 id="HDFS-使用"><a href="#HDFS-使用" class="headerlink" title="HDFS 使用"></a>HDFS 使用</h2><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><p>HDFS本质是一个文件系统，所以跟Linux 的文件系统使用类似，也是增删改查，不过这里的“改”不是update，而是truncate，也就是文件截断，HDFS 不支持update操作，这与它读多写少的特性相对应。<br>对HDFS的操作可以通过shell，web 界面，libhdfs (C API)或WebHDFS (REST API)来操作。HDFS里的文件跟linux 文件类似，也有own user，group，也有读写执行权限的划分，不过这里的可执行权限只对目录有用，意思是是否有权限对该目录的子目录或文件有可读权限。每一个文件的操作命令都会进行Permission Checks，不通过则fail。<br>HDFS的具体shell命令略，可参考<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="noopener">FileSystemShell</a>。</p>
<h3 id="Quotas-设置"><a href="#Quotas-设置" class="headerlink" title="Quotas 设置"></a>Quotas 设置</h3><p>HDFS支持 administrator对quota的设置，包括：</p>
<ul>
<li>name quota: 指定目录下文件或者目录的数量限制。</li>
<li>space quota: 指定目录下文件的占用空间限制。</li>
<li>Storage Type Quotas：指定目录下Storage Type的限制。</li>
</ul>
<p>administrator可以通过命令行或其他的方式进行设置。</p>
<h3 id="透明加密"><a href="#透明加密" class="headerlink" title="透明加密"></a>透明加密</h3><p>透明加密主要是防止application与HDFS之间进行端到端的数据传输时的数据安全，不需要更改user application的任何代码。<br>详细参考<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html" target="_blank" rel="noopener">TransparentEncryption</a>。</p>
<h2 id="HDFS-HA"><a href="#HDFS-HA" class="headerlink" title="HDFS HA"></a>HDFS HA</h2><p>这里的HA主要是指namenode的HA。hadoop文档提供了两种方法，本质其实是一样的，因为namenode主要依靠FsImage和EditLog两个文件管理DataNode的数据，要想保证新的namenode能随时替换，就要保证这两个文件的一致性，FsImage是存储在磁盘上还好说，EditLog在内存里随时变化，就要保证两个namenode的EditLog文件是一样的。两个方案如下：</p>
<ul>
<li>QJM：the Quorum Journal Manager，这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致,配合zookeeper可以实现自动切换,推荐使用。</li>
<li>NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），Active NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致,缺点是如果namenode或者standby namenode与NFS磁盘之间的网络出了问题，HA即失效。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-2-2-HDFS-HA.jpg" alt="HDFS-HA"></p>
<p>以上是QJM HA的典型的结构图。集群中共有两个namenode(简称NN)，其中只有一个是active状态，另一个是standby状态。active 的NN负责响应DN(datanode)的请求，为了最快的切换为active状态，standby状态的NN同样也连接到所有的datenode上获取最新的块信息(blockmap)。<br>active NN会把元数据的修改(edit log)发送到多数的journal节点上(2n+1个journal节点，至少写到n+1个上)，standby NN从journal节点上读取edit log，并实时的合并到自己的namespace中。另外standby NN连接所有DN，实时的获取最新的blockmap。这样，一旦active的NN出现故障，standby NN可以立即切换为active NN.</p>
<p>具体配置参考<a href="http://www.bijishequ.com/detail/373246" target="_blank" rel="noopener">HADOOP(二):HDFS 高可用原理</a></p>
<h2 id="HDFS-认证与授权"><a href="#HDFS-认证与授权" class="headerlink" title="HDFS 认证与授权"></a>HDFS 认证与授权</h2><p>为了确保数据安全，认证授权这一块hadoop也下了一番功夫，一般来说，需要经历一下几个阶段：</p>
<ul>
<li>认证</li>
<li>proxy user</li>
<li>service level Authorization</li>
<li>第三方权限控制Ranger</li>
<li>Hadoop POSIX ACLs</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-02-02Hadoop%20Range.png" alt="hadoop-auth"></p>
<p>认证部分有两种方式，simple和kerberos，simple不做任何处理，会由操作系统层获取用户，客户端可以通过设置环境变量HADOOP_USER_NAME来伪装用户。kerberos认证对集群里的所有机器都分发了keytab，使得集群机器进程之间不能随便访问。<br>代理部分：当客户端访问hadoop时，并不想以当前进程用户去调用，上层应用一般有自己一套用户管理体系，所以hadoop提供代理机制，让进程用户可以代理登录用户提交请求。然而，如果对代理用户不加以控制的话，那权限便相对于无限放大，比如代理超级用户：hdfs，yarn等，所以对于进程用户会设置可在哪些主机提交请求和代理哪些用户组成员。<br>权限控制首先经过Service Level Authorization，检测服务级别权限。<br>比如哪些用户可以连接namenode，resourcemanager，属于服务级别的acl控制。参考<a href="https://www.iteblog.com/archives/983.html" target="_blank" rel="noopener">Hadoop服务层授权控制</a><br>接着由Ranger进行目录，队列等资源的权限管控, 属于更细粒度的权限控制。如果ranger没有策略控制，则进入原始HDFS文件系统权限或者MR权限控制。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Welcome to Apache Hadoop</a></p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="noopener">HDFS Users Guide</a></p>
<p><a href="http://pangjiuzala.github.io/2015/08/02/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Hadoop%E9%9B%86%E7%BE%A4%E5%92%8C%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">深入理解Hadoop集群和网络</a></p>
<p><a href="http://www.cnblogs.com/zhangmingcheng/p/6406792.html" target="_blank" rel="noopener">Hadoop2.7.3 HA高可靠性集群搭建</a></p>
<p><a href="http://blog.csdn.net/mmd0308/article/details/74674524" target="_blank" rel="noopener">Hadoop之HDFS分布式文件系统NameNode及Secondary NameNode详解</a></p>
<p><a href="http://komi.leanote.com/post/Hadoop-Ranger%E6%9D%83%E9%99%90%E6%B5%81%E7%A8%8B" target="_blank" rel="noopener">Hadoop 访问管理</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/01/26/centos7-install-cdh5.14/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/26/centos7-install-cdh5.14/" itemprop="url">
                  Bigdata-- Centos7 安装CDH5.14记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-26T16:57:10+08:00">
                2018-01-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/26/centos7-install-cdh5.14/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/01/26/centos7-install-cdh5.14/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>目标是搭建一个CDH 的测试环境，图方便在网上找了一些中文的搭建博客，结果问题百出，还是回到官网浏览，找到对应的安装文档，花了一点时间搭建，这里记录一下。<br>系统配置如下：</p>
<ul>
<li>centos7.2 minimal,8G内存，100G磁盘(master节点)</li>
<li>centos7.2 minimal,4G内存，100G磁盘(node节点)</li>
<li>centos7.2 minimal,4G内存，100G磁盘(node节点)</li>
<li>centos7.2 minimal,4G内存，100G磁盘(node节点)<br>搭建的cdh为最新的CDH5.14，感觉master内存还是太少，有点吃力。</li>
</ul>
<h2 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h2><p>CDH的搭建有多种方法，一般来说，都是先搭建Cloudera Manager,然后利用Cloudera Manager搭建CDH，如果是测试环境，可以直接利用Cloudera Manager自动化安装，这种方式使用内嵌的PostgreSQL作为metadata等数据的存储，不适于生产环境。生产环境中一般会使用Mysql或其他独立搭建的数据库（当然要做HA），所以我们会先搭建一个Mysql数据库备用。</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>在所有节点上都要执行的工作，包括host设置，设置yum repo，无密钥登录，ntp设置，安装jdk,关闭防火墙等等。</p>
<ul>
<li>修改 hostname，执行命令 hostname NAME,并修改 /etc/hostname文件。</li>
<li><p>修改/etc/hosts文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.10.10.77    cdh1</span><br><span class="line">10.10.10.78    cdh2</span><br><span class="line">10.10.10.79    cdh3</span><br><span class="line">10.10.10.82    cdh4</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭防火墙.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop firewalld</span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">``` </span><br><span class="line">- 所有节点设置无密钥登录,命令大致如下，将最终的authorized_keys再覆盖回所有节点：</span><br><span class="line">```bash</span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line">$ cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line">$ chmod 600 authorized_keys</span><br><span class="line">$ scp authorized_keys root@cdh2:~/.ssh/</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置 yum repo.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo  <span class="comment"># 阿里yum源</span></span><br><span class="line">$ curl -o /etc/yum.repos.d/cloudera-manager.repo https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo  <span class="comment"># cloudera yum源</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>ntp设置，安装ntp，编辑crontab定时同步。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ntpd</span><br><span class="line">$ ntpdate time1.aliyun.com</span><br><span class="line">$ crontab e</span><br><span class="line">30 02 * * *  ntpdate time1.aliyun.com</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装jdk.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install oracle-j2sdk1.7 -y</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭SElinux,修改/etc/selinux/config为disabled，重启生效。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#       enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#       permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#       disabled - SELinux is fully disabled.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># SELINUXTYPE= type of policy in use. Possible values are:</span></span><br><span class="line"><span class="comment">#       targeted - Only targeted network daemons are protected.</span></span><br><span class="line"><span class="comment">#       strict - Full SELinux protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="master节点安装与配置Mysql"><a href="#master节点安装与配置Mysql" class="headerlink" title="master节点安装与配置Mysql"></a>master节点安装与配置Mysql</h3><ul>
<li><p>安装mysql.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">$ sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">$ yum update</span><br><span class="line">$ sudo yum install mysql-server</span><br><span class="line">$ sudo systemctl start mysqld</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除/var/lib/mysql/ib_logfile0 和 /var/lib/mysql/ib_logfile1文件，并配置/etc/my.conf如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">transaction-isolation = READ-COMMITTED</span><br><span class="line"><span class="comment"># Disabling symbolic-links is recommended to prevent assorted security risks;</span></span><br><span class="line"><span class="comment"># to do so, uncomment this line:</span></span><br><span class="line"><span class="comment"># symbolic-links = 0</span></span><br><span class="line"></span><br><span class="line">key_buffer_size = 32M</span><br><span class="line">max_allowed_packet = 32M</span><br><span class="line">thread_stack = 256K</span><br><span class="line">thread_cache_size = 64</span><br><span class="line">query_cache_limit = 8M</span><br><span class="line">query_cache_size = 64M</span><br><span class="line">query_cache_type = 1</span><br><span class="line"></span><br><span class="line">max_connections = 550</span><br><span class="line"><span class="comment">#expire_logs_days = 10</span></span><br><span class="line"><span class="comment">#max_binlog_size = 100M</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#log_bin should be on a disk with enough free space. Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your system</span></span><br><span class="line"><span class="comment">#and chown the specified folder to the mysql user.</span></span><br><span class="line">log_bin=/var/lib/mysql/mysql_binary_log</span><br><span class="line"></span><br><span class="line"><span class="comment"># For MySQL version 5.1.8 or later. For older versions, reference MySQL documentation for configuration help.</span></span><br><span class="line">binlog_format = mixed</span><br><span class="line"></span><br><span class="line">read_buffer_size = 2M</span><br><span class="line">read_rnd_buffer_size = 16M</span><br><span class="line">sort_buffer_size = 8M</span><br><span class="line">join_buffer_size = 8M</span><br><span class="line"></span><br><span class="line"><span class="comment"># InnoDB settings</span></span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">innodb_flush_log_at_trx_commit  = 2</span><br><span class="line">innodb_log_buffer_size = 64M</span><br><span class="line">innodb_buffer_pool_size = 4G</span><br><span class="line">innodb_thread_concurrency = 8</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">innodb_log_file_size = 512M</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line"><span class="built_in">log</span>-error=/var/<span class="built_in">log</span>/mysqld.log</span><br><span class="line">pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line"></span><br><span class="line">sql_mode=STRICT_ALL_TABLES</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装 MySQL JDBC Driver从<a href="http://www.mysql.com/downloads/connector/j/5.1.html" target="_blank" rel="noopener">该页面</a>下载对应tar包，解压。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxvf mysql-connector-java-5.1.31.tar.gz</span><br><span class="line">$ mkdir -p /usr/share/java/</span><br><span class="line">$ cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建对应的数据库。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database database DEFAULT CHARACTER SET utf8;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; grant all on database.* TO <span class="string">'user'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'password'</span>;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>数据库的名字，用户名等可以参考除了以下列出的，还需创建oozie的数据库以及用户名密码.<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180129200028-mysql-role.jpg" alt="mysql"></p>
<h3 id="Cloudera-Manager-安装"><a href="#Cloudera-Manager-安装" class="headerlink" title="Cloudera Manager 安装"></a>Cloudera Manager 安装</h3><ul>
<li><p>在master节点安装Cloudera Manager Server并启动。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install cloudera-manager-daemons cloudera-manager-server</span><br><span class="line">$ systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure>
</li>
<li><p>在master和node节点安装Cloudera Manager Agent。修改 /etc/cloudera-scm-agent/config.ini 中的server_host为master的IP。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install cloudera-manager-agent cloudera-manager-daemons</span><br><span class="line">$ systemctl start cloudera-scm-agent</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CDH-安装"><a href="#CDH-安装" class="headerlink" title="CDH 安装"></a>CDH 安装</h3><p>进入 Cloudera Manager的console，<a href="http://Server" target="_blank" rel="noopener">http://Server</a> host:7180,登录后便可以进入CDH的安装部署了。</p>
<p>该过程没有截图，且因为web界面看着比较直白不在赘述。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_install_path_b.html#id_z2h_pnm_25" target="_blank" rel="noopener">Installation Path B - Installation Using Cloudera Manager Parcels or Packages</a></p>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/hue_dbs_mysql.html#concept_tq4_tbt_zw" target="_blank" rel="noopener">Connect Hue to MySQL or MariaDB</a></p>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_troubleshooting.html#cmig_topic_19" target="_blank" rel="noopener">Troubleshooting Installation and Upgrade Problems</a></p>
<p><a href="https://segmentfault.com/a/1190000011341408" target="_blank" rel="noopener">离线部署 Cloudera Manager 5 和 CDH 5.12.1 及使用 CDH 部署 Hadoop 集群服务</a></p>
<p><a href="https://www.cmgine.com/archives/19107.html" target="_blank" rel="noopener">Centos 7安装CDH 5.13.0总结</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2018/01/20/create-rpm-for-python-flask-application/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/20/create-rpm-for-python-flask-application/" itemprop="url">
                  Ops-- 将Flask APP 打包为rpm包
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-20T15:57:10+08:00">
                2018-01-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/20/create-rpm-for-python-flask-application/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/01/20/create-rpm-for-python-flask-application/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>为了实现无人值守安装，需要将我们用Python研发的一个WEB做成一个RPM 包，这里简单记录一下。<br>先简单介绍下需要打包的这个Python应用，是我们为了安装CDH 大数据平台做的一个辅助WEB，使用Flask开发，除了用cherry起的一个WEB 服务外，还有一个celery的worker 进程。 </p>
<h2 id="构建思路"><a href="#构建思路" class="headerlink" title="构建思路"></a>构建思路</h2><ul>
<li>首先是该Python应用的依赖包，有两种解决方案，一种是全部打包到一个RPM包里，简单粗暴，不容易起冲突，但是不灵活，文件大，每做一次升级比较麻烦，第二种方案是将该应用的依赖包做成RPM包（其实都有现成的），在SPEC文件中注明依赖包，这样在RPM 安装时会自动安装依赖包，这种方案比较灵活，不过需要花时间去找到所有依赖包的RPM 包并放到YUM REPO中。我们采用的是第二种方案。</li>
<li>因为要起两个服务，所以要写两个启动文件。</li>
<li>Python 的setuptools 有一个 python setup.py bdist_rpm 命令用于构建rpm包，不过还是需要自己定制SPEC文件，所以这里直接使用rpmbuild。</li>
<li>本次构建的系统环境是centos6.9。</li>
</ul>
<h2 id="具体实践"><a href="#具体实践" class="headerlink" title="具体实践"></a>具体实践</h2><ul>
<li><p>安装rpmbuild</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y rpm-build</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用普通用户并修改topdir目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ useradd rpmbuilder</span><br><span class="line">$ su - rpmbuilder</span><br><span class="line">$ vim ~/.rpmmacros    <span class="comment"># 修改工作目录</span></span><br><span class="line">  %_topdir        /home/rpmbuilder/rpmbuild</span><br><span class="line"></span><br><span class="line">$ mkdir -pv ~/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>简单介绍下工作目录中的几个目录的作用：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20180120rpm-dir.jpg" alt="rpm-dir"></p>
<ul>
<li><p>准备源码文件，主要包括三个文件，一个是Flask源码文件全部打包压缩成一个tar.gz包，还有两个启动文件。将这三个文件放到SOURCES目录。启动文件示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">. /etc/rc.d/init.d/<span class="built_in">functions</span></span><br><span class="line"></span><br><span class="line">runuser=root</span><br><span class="line">prog=cdhboot-web</span><br><span class="line">worker_num=10</span><br><span class="line"><span class="built_in">export</span> C_FORCE_ROOT=True</span><br><span class="line"><span class="built_in">exec</span>=<span class="string">"/opt/cdhboot/supervisor/server_cherrypy.py"</span>  <span class="comment"># 我们将所有源码文件安装到/opt/cdhboot/目录，后面会提到。</span></span><br><span class="line">pidfile=<span class="string">"/var/run/cdhboot/<span class="variable">$prog</span>.pid"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">start</span></span>() &#123;</span><br><span class="line">    [ -f <span class="variable">$exec</span> ] || <span class="built_in">exit</span> 5</span><br><span class="line">    <span class="built_in">echo</span> -n $<span class="string">"Starting <span class="variable">$prog</span>: "</span></span><br><span class="line">    daemon --user <span class="variable">$runuser</span> --pidfile <span class="variable">$pidfile</span> <span class="string">"python <span class="variable">$exec</span> &amp;&gt;/dev/null &amp; echo \$! &gt; <span class="variable">$pidfile</span>"</span></span><br><span class="line">    retval=$?</span><br><span class="line">    <span class="built_in">echo</span></span><br><span class="line">    <span class="built_in">return</span> <span class="variable">$retval</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">stop</span></span>() &#123;</span><br><span class="line">    <span class="built_in">echo</span> -n $<span class="string">"Stopping <span class="variable">$prog</span>: "</span></span><br><span class="line">    killproc -p <span class="variable">$pidfile</span> <span class="variable">$prog</span></span><br><span class="line">    retval=$?</span><br><span class="line">    <span class="built_in">echo</span></span><br><span class="line">    <span class="built_in">return</span> <span class="variable">$retval</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">restart</span></span>() &#123;</span><br><span class="line">    stop</span><br><span class="line">    start</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">reload</span></span>() &#123;</span><br><span class="line">    restart</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="title">force_reload</span></span>() &#123;</span><br><span class="line">    restart</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">rh_status</span></span>() &#123;</span><br><span class="line">    status -p <span class="variable">$pidfile</span> <span class="variable">$prog</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">rh_status_q</span></span>() &#123;</span><br><span class="line">    rh_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></span><br><span class="line">    start)</span><br><span class="line">        rh_status_q &amp;&amp; <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        rh_status_q || <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line">    restart)</span><br><span class="line">        <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line">    reload)</span><br><span class="line">        rh_status_q || <span class="built_in">exit</span> 7</span><br><span class="line">        <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line">    force-reload)</span><br><span class="line">        force_reload</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        rh_status</span><br><span class="line">    condrestart|try-restart)</span><br><span class="line">        rh_status_q || <span class="built_in">exit</span> 0</span><br><span class="line">        restart</span><br><span class="line">        ;;</span><br><span class="line">    *)</span><br><span class="line">        <span class="built_in">echo</span> $<span class="string">"Usage: <span class="variable">$0</span> &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload&#125;"</span></span><br><span class="line">        <span class="built_in">exit</span> 2</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"><span class="built_in">exit</span> $?</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写SPEC文件，SPEC文件有几个部分组成，也代表着rpm打包时的几个步骤。先看下rpm打包的四个步骤：</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/rpmbulld-step-20180122133542.jpg" alt="rpm-build-step"><br>对应的SPEC文件示例如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一部分：自定义的变量，通常是包名，版本等信息</span></span><br><span class="line">%define name cdhboot</span><br><span class="line">%define version 0.1</span><br><span class="line">%define unmangled_version 0.1</span><br><span class="line">%define unmangled_version 0.1</span><br><span class="line">%define release 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部分：定义rpm包的信息，三个源文件，依赖包，以及BuildRoot</span></span><br><span class="line">Summary: cdh boot</span><br><span class="line">Name: %&#123;name&#125;</span><br><span class="line">Version: %&#123;version&#125;</span><br><span class="line">Release: %&#123;release&#125;</span><br><span class="line">Source0: %&#123;name&#125;-%&#123;unmangled_version&#125;.tar.gz </span><br><span class="line">Source1: cdhboot-worker</span><br><span class="line">Source2: cdhboot-web</span><br><span class="line">License: MIT</span><br><span class="line">Group: Development/Libraries</span><br><span class="line">BuildRoot: /root/rpmbuild/  </span><br><span class="line">Prefix: %&#123;_prefix&#125;</span><br><span class="line">BuildArch: noarch</span><br><span class="line">Vendor: pekingzcc &lt;pekingzcc@gmail.com&gt;</span><br><span class="line">Url: https://github.com/zhangchenchen</span><br><span class="line">Requires: python-celery,python-mongoengine,python-prettytable,python-cherrypy,python-argparse,pytz,python-flask,python-flask-login  <span class="comment"># 依赖包，使用yum安装时会先下载安装依赖包</span></span><br><span class="line">%description</span><br><span class="line">WEB TO BOOT CDH</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三部分：准备阶段，%setup是一个宏命令，解压缩包到cdhboot目录，并cd到该目录下。</span></span><br><span class="line">%prep</span><br><span class="line">%setup -n %&#123;name&#125;-%&#123;unmangled_version&#125; -n cdhboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四部分：安装之前的操作，添加一个sysadmin用户</span></span><br><span class="line">%pre</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> == 1 ];<span class="keyword">then</span></span><br><span class="line">    /usr/sbin/useradd  sysadmin -s /sbin/nologin 2&gt; /dev/null</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五部分：安装阶段，创建相应目录。并将源文件复制到相应目录。%&#123;__install&#125;是一个宏命令，类似于cp命令。</span></span><br><span class="line">%install</span><br><span class="line">mkdir -p %&#123;buildroot&#125;/opt/cdhboot</span><br><span class="line">cp -r ./* %&#123;buildroot&#125;/opt/cdhboot/</span><br><span class="line">mkdir -p  %&#123;buildroot&#125;/var/run/cdhboot</span><br><span class="line">mkdir -p  %&#123;buildroot&#125;/var/<span class="built_in">log</span>/cdhboot</span><br><span class="line">%&#123;__install&#125; -p -D -m 0755 %&#123;SOURCE1&#125; %&#123;buildroot&#125;/etc/rc.d/init.d/cdhboot-worker</span><br><span class="line">%&#123;__install&#125; -p -D -m 0755 %&#123;SOURCE2&#125; %&#123;buildroot&#125;/etc/rc.d/init.d/cdhboot-web</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第六部分：安转之后的操作，加入开机启动服务</span></span><br><span class="line">%post</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> == 1 ];<span class="keyword">then</span></span><br><span class="line">    /sbin/chkconfig --add cdhboot-worker</span><br><span class="line">    /sbin/chkconfig cdhboot-worker on</span><br><span class="line">    /sbin/chkconfig --add cdhboot-web</span><br><span class="line">    /sbin/chkconfig cdhboot-web on</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第七部分：卸载之后的操作，删除sysadmin用户，并停止服务 </span></span><br><span class="line">%preun</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> == 0 ];<span class="keyword">then</span></span><br><span class="line">        /usr/sbin/userdel  sysadmin 2&gt; /dev/null</span><br><span class="line">        /etc/init.d/cdhboot-worker stop &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">        /etc/init.d/cdhboot-web stop &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第八部分：构建完成后删除临时构建目录内容</span></span><br><span class="line">%clean</span><br><span class="line">rm -rf <span class="variable">$RPM_BUILD_ROOT</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第九部分：文件部分，但凡上文构建过程中出现的文件或目录，这里都要对这些文件或目录注明属性</span></span><br><span class="line">%files </span><br><span class="line">%defattr(-,root,root)</span><br><span class="line">%attr(0755,root,root) /etc/rc.d/init.d/cdhboot-worker</span><br><span class="line">%attr(0755,root,root) /etc/rc.d/init.d/cdhboot-web</span><br><span class="line">/opt</span><br><span class="line">/var/run</span><br><span class="line">/var/<span class="built_in">log</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>测试SPEC文件。为了测试SPEC文件，我们可以分阶段的执行构建命令。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rpmbuild -bp cdhboot.spec 制作到%prep段</span><br><span class="line">rpmbuild -bc cdhboot.spec 制作到%build段</span><br><span class="line">rpmbuild -bi cdhboot.spec 执行 spec 文件的 <span class="string">"%install"</span> 阶段 (在执行了 %prep 和 %build 阶段之后)。这通常等价于执行了一次 <span class="string">"make install"</span></span><br><span class="line">rpmbuild -bb cdhboot.spec 制作二进制包</span><br><span class="line">rpmbuild -ba cdhboot.spec 表示既制作二进制包又制作src格式包,即全过程。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通过分阶段的构建来测试对应部分的编写正确性，同时，也可以去临时目录BUILDROOT 下查看构建的文件，临时目录BUILDROOT在构建阶段相当于安装时机器的根目录。<br>构建完成后再RPMS目录下可以看到构建成功的rpm包。</p>
<p>之后可以对rpm包进行签名，就可以放到yum repo中发布了。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://stackoverflow.com/questions/42286786/how-to-create-a-rpm-for-python-application" target="_blank" rel="noopener">How to create a rpm for python application</a></p>
<p><a href="http://blog.51cto.com/nmshuishui/1583117" target="_blank" rel="noopener">使用rpm-build制作nginx的rpm包</a></p>
<p><a href="http://www.voidcn.com/article/p-zvfjwgek-up.html" target="_blank" rel="noopener">记录自己将Python程序打包成rpm包的过程</a></p>
<p><a href="https://wenchao.ren/archives/549" target="_blank" rel="noopener">rpmbuild spec文件的编写,以及rpm包的打包</a></p>
<p><a href="https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn" target="_blank" rel="noopener">How to create an RPM package/zh-cn</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="pekingzcc" />
          <p class="site-author-name" itemprop="name">pekingzcc</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">100</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">48</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/zhangchenchen" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">pekingzcc</span>
</div>


<div class="powered-by">
  powered by <a class="theme-link" href="https://hexo.io">Hexo</a> 
</div>

<div class="theme-info">
  theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'pekingzcc';
      var disqus_identifier = 'index.html';

      var disqus_title = "";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      

    </script>
  









  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

  


</body>
</html>
