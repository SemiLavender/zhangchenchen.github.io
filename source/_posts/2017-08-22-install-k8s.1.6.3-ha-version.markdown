---
layout: post
title:  "Kubernetes-- 手动安装高可用k8s1.6问题汇总"
date:   2017-08-22 14:25:20
tags: 
  - kubernetes
---


## kubernetes HA 整体架构

k8s的HA相对于[openstack的HA](https://zhangchenchen.github.io/2017/04/14/openstack-ha/)要简单很多，主要包括以下三各方面：

- etcd的HA:创建HA集群,如果还不放心，可以使用分布式存储系统
- apiserver(无状态服务)的HA:双活模式，前面加一个load balance
- controller manager 和 scheduler的HA:主备模式。

![k8s-ha](http://7xrnwq.com1.z0.glb.clouddn.com/20170822-k8s-ha.jpg)

博主是按照[Kubernetes 1.6 高可用集群部署](http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署)这篇博客一步步安装下来的，基于Kubernetes二进制包手动部署一个高可用的Kubernetes 1.6集群，将启用ApiServer的TLS双向认证和RBAC授权等安全机制，当然，也可以利用kubeadm创建一个k8s cluster后再扩展成高可用的，参考[一步步打造基于Kubeadm的高可用Kubernetes集群](http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/)。
安装过程不再赘述，参考上述博文，主要讲下期间遇到的问题以及解决方法。


## etcd高可用集群部署

在该过程中碰到的一个坑是，在创建tls密钥和证书的过程中。用的工具是cfssl,创建etcd证书签名请求配置文件的时候需要指定node节点的IP，当时希望可以外网访问，就使用的floating-ip,在etcd的systemd unit文件中自然也就使用了floating-ip，然而etcd却一直起不来，查看日志报错： listen tcp 172.16.21.55:2380: bind: cannot assign requested address.

监听端口失败，恍然大悟，openstack中的floating-ip是不会在虚拟机上创建一个新网卡的，而是通过l3-agent的转发实现。将floating-ip更改为内网IP 后问题解决。


##  Kubernetes各组件TLS证书和密钥

除了etcd是使用tls双向认证外，这里apiserver也启用了双向认证，这样的话，凡是与apiserver通信的组件都要创建对应的证书与密钥,所有的需要创建的组件包括：

- kube-apiserver
- kubernetes-admin：RBAC相关，该证书拥有访问kube-apiserver的所有权限。
- kube-controller-manager
- kube-scheduler
- kubelet:每个节点都要配置
- kube-proxy：每个节点都要配置


## Kubernetes Master集群部署

我们这里用openstack创建了一个load balance用于apiserver的高可用，如果是在裸机上，也有多种方案，比如，可以使用haproxy+keepalived的方案实现，也可以直接在各节点用nginx做反向代理，参考[kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明](https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS)。

## Kubernetes Node节点部署

在部署Pod Network插件flannel碰到一个问题，执行完create命令后，pod一直处于containercreating状态，describe发现报错信息为：error syncing pod，没有多余的报错信息，而且容器压根就没有创建起来。挣扎了许久，最后直接在/var/log/messaging中暴力搜索了一下syncing pod，终于找到有用的信息：
```bash
.....
Aug 22 13:12:33 host-10-10-10-52 dockerd: time="2017-08-22T13:12:33.841538881+08:00" level=error msg="Handler for GET /v1.24/images/gcr.io/google_containers/pause-amd64:3.0/json returned error: No such image: gcr.io/google_containers/pause-amd64:3.0"
 
```

通过翻墙将该镜像pull到本地后，一切解决。关于pause container的作用，直接引用[What is the role of 'pause' container?](https://groups.google.com/forum/#!topic/kubernetes-users/jVjv0QK4b_o)

>>>The pause container is a container which holds the network namespace for the pod. It does nothing 'useful'. (It's actually just a little bit of assembly that goes to sleep and never wakes up)
This means that your 'apache' container can die, and come back to life, and all of the network setup will still be there. Normally if the last process in a network namespace dies the namespace would be destroyed and creating a new apache container would require creating all new network setup. With pause, you'll always have that one last thing in the namespace.



## 参考文章


[Building High-Availability Clusters](https://kubernetes.io/docs/admin/high-availability/)

[Kubernetes 1.6 高可用集群部署](http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署)

[ssl 双向认证和单向认证原理](http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html)

[kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明](https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS)

***本篇文章由[pekingzcc](https://zhangchenchen.github.io/)采用[知识共享署名-非商业性使用 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可,转载请注明。***


 ***END***